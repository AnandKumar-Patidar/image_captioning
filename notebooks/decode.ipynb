{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input weights W_{hx}:\n",
      " Parameter containing:\n",
      "tensor([[ 0.5384, -0.2044,  0.1479,  0.2199],\n",
      "        [ 0.0697, -0.5088,  0.2661, -0.1335],\n",
      "        [-0.3623,  0.4109, -0.3424, -0.2226]], requires_grad=True)\n",
      "Hidden weights W_{hh}:\n",
      " Parameter containing:\n",
      "tensor([[ 0.5343,  0.2117, -0.1317],\n",
      "        [ 0.4909,  0.0283, -0.3987],\n",
      "        [ 0.0164, -0.3089,  0.0196]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create an RNN\n",
    "rnn = nn.RNN(input_size=4, hidden_size=3, num_layers=1)\n",
    "\n",
    "# Check initial weights\n",
    "print(\"Input weights W_{hx}:\\n\", rnn.weight_ih_l0)  # Weights for input to hidden\n",
    "print(\"Hidden weights W_{hh}:\\n\", rnn.weight_hh_l0)  # Weights for hidden to hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "('weight_ih_l0', Parameter containing:\n",
      "tensor([[ 0.5384, -0.2044,  0.1479,  0.2199],\n",
      "        [ 0.0697, -0.5088,  0.2661, -0.1335],\n",
      "        [-0.3623,  0.4109, -0.3424, -0.2226]], requires_grad=True))\n",
      "('weight_hh_l0', Parameter containing:\n",
      "tensor([[ 0.5343,  0.2117, -0.1317],\n",
      "        [ 0.4909,  0.0283, -0.3987],\n",
      "        [ 0.0164, -0.3089,  0.0196]], requires_grad=True))\n",
      "('bias_ih_l0', Parameter containing:\n",
      "tensor([ 0.0477,  0.2878, -0.2912], requires_grad=True))\n",
      "('bias_hh_l0', Parameter containing:\n",
      "tensor([-0.1890, -0.0997, -0.1595], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for i in rnn.named_parameters():\n",
    "    print(type(i))\n",
    "for i in rnn.named_parameters():\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[ 0.5384, -0.2044,  0.1479,  0.2199],\n",
      "        [ 0.0697, -0.5088,  0.2661, -0.1335],\n",
      "        [-0.3623,  0.4109, -0.3424, -0.2226]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in rnn.named_parameters():\n",
    "    print(i[0], i[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.6403, -0.4580,  0.0760,  0.2228],\n",
      "        [ 0.7605,  0.2104, -0.1391, -0.1490],\n",
      "        [-0.5899, -0.1240,  0.1980, -0.5080]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0377,  0.6641,  0.1304],\n",
      "        [-0.6605,  0.9089, -0.7736],\n",
      "        [ 0.3250,  0.9797, -0.7253]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize weights with Xavier uniform distribution\n",
    "for name, param in rnn.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        nn.init.xavier_uniform_(param)\n",
    "        print(param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Initializing Weights with a Uniform Distribution\n",
    "\n",
    "Initializing weights using a **uniform distribution** offers several important benefits, particularly in the context of neural networks. Let’s break down the main advantages:\n",
    "\n",
    "#### 1. Breaking Symmetry\n",
    "\n",
    "One of the primary reasons for initializing weights with a uniform distribution (or any random distribution) is to **break symmetry** between the neurons. If all weights are initialized to the same value (e.g., zeros), each neuron in a layer will receive the same gradients and update in the same way during training, effectively making them learn the same features. Random initialization with a uniform distribution ensures that neurons start with different weights, allowing them to learn different aspects of the data.\n",
    "\n",
    "- **Benefit**: Different neurons can learn different features, leading to a more expressive and powerful network.\n",
    "\n",
    "#### 2. Ensuring Appropriate Scale of Weights\n",
    "\n",
    "A uniform distribution allows control over the range of the initial weights, which can help keep the initial activations and gradients at a manageable scale, preventing problems like vanishing or exploding gradients. Proper initialization, particularly with small random values, helps ensure that the input signals neither shrink nor grow too much as they propagate through the network.\n",
    "\n",
    "- **Benefit**: Reduces the risk of vanishing or exploding gradients, leading to more stable and efficient training.\n",
    "\n",
    "#### 3. Faster Convergence\n",
    "\n",
    "Random weight initialization using a uniform distribution, especially when coupled with techniques like **Xavier/Glorot** or **He/Kaiming** initialization, helps the network converge faster by providing weights that are well-scaled for the specific activation functions in use (e.g., sigmoid, ReLU). These initialization techniques are designed to keep the variance of the outputs consistent across layers, which can significantly improve the training speed and convergence.\n",
    "\n",
    "- **Benefit**: Improved training speed and faster convergence to an optimal solution.\n",
    "\n",
    "#### 4. Flexibility\n",
    "\n",
    "Using a uniform distribution offers flexibility in controlling the range of initial weights. By specifying the bounds (e.g., `[-a, a]`), you can ensure the initial weights are not too large or too small, which can help avoid large fluctuations or vanishing signals during forward and backward propagation.\n",
    "\n",
    "- **Benefit**: Control over the range of initial weights, reducing the risk of extreme initial values that can destabilize training.\n",
    "\n",
    "#### 5. Good for Large Networks\n",
    "\n",
    "Uniform initialization works well in practice for large networks, as it ensures that each neuron starts with a different weight but within a controlled range. This is especially important in deep neural networks, where improper weight initialization can cause problems as the signals pass through many layers.\n",
    "\n",
    "- **Benefit**: Uniform initialization provides a practical and scalable solution for initializing weights in large, deep neural networks.\n",
    "\n",
    "### Common Methods Based on Uniform Distribution\n",
    "\n",
    "Several commonly used weight initialization techniques are based on uniform distributions. These methods adjust the range of the uniform distribution to account for the size of the input and output layers:\n",
    "\n",
    "1. **Xavier/Glorot Uniform Initialization**:\n",
    "   - Uses a uniform distribution with the range dependent on the number of input and output neurons.\n",
    "   - Designed for use with sigmoid and tanh activation functions.\n",
    "   \n",
    "      \\[\n",
    "      $W \\sim \\mathcal{U}\\left( -\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}} \\right)\n",
    "      \\]\n",
    "      where \\( n \\) is the number of input and output units in the layer.\n",
    "\n",
    "2. **He/Kaiming Uniform Initialization**:\n",
    "   - Uses a uniform distribution with the range adjusted for ReLU and variants.\n",
    "   \n",
    "   \\[\n",
    "   W \\sim \\mathcal{U}\\left( -\\sqrt{\\frac{6}{n}}, \\sqrt{\\frac{6}{n}} \\right)\n",
    "   \\]\n",
    "   where \\( n \\) is the number of input units in the layer.\n",
    "\n",
    "These methods ensure that the variance of the activations remains consistent across layers, which is critical for efficient training.\n",
    "\n",
    "### Summary of Benefits\n",
    "\n",
    "1. **Breaking Symmetry**: Ensures neurons have unique weights, allowing them to learn different features.\n",
    "2. **Appropriate Weight Scale**: Helps avoid vanishing or exploding gradients by controlling the range of initial weights.\n",
    "3. **Faster Convergence**: Properly initialized weights lead to faster and more stable convergence during training.\n",
    "4. **Flexibility**: You can control the bounds of the uniform distribution to fit the network’s needs.\n",
    "5. **Scalable to Large Networks**: Uniform initialization is practical for large, deep networks and can be tailored for different activation functions.\n",
    "\n",
    "In conclusion, initializing weights with a uniform distribution ensures that the neural network can learn efficiently from the start, prevents various common issues during training, and improves convergence speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7095,  0.3367, -0.3368,  1.2858,  1.0533, -2.2277,  0.8800,\n",
       "          -0.3742, -0.7403,  0.0883],\n",
       "         [ 0.5700,  0.0425, -0.3584,  2.2801,  0.1727, -0.7082,  0.0138,\n",
       "           1.0876,  0.3305,  1.0911],\n",
       "         [-0.3166,  0.2255,  0.6320,  1.1624,  0.7219,  0.3716, -0.5637,\n",
       "           0.7088,  0.1332, -0.9087]],\n",
       "\n",
       "        [[ 0.2929, -0.3405, -0.7353,  0.2625, -0.2958,  1.9104, -0.9117,\n",
       "          -0.1829, -0.1219, -0.8041],\n",
       "         [ 1.1385, -0.6271,  0.2522, -0.6451, -0.4261, -2.2981,  1.1322,\n",
       "          -0.9697, -0.0300,  0.0631],\n",
       "         [-1.2156,  0.8823,  1.6000, -1.7288,  2.1517,  1.2736,  0.3070,\n",
       "           1.3770, -1.0922,  1.1184]],\n",
       "\n",
       "        [[ 0.3145, -0.3453,  1.0731, -0.6722, -0.0336, -0.9665,  2.2367,\n",
       "          -0.6329,  0.7638, -0.1410],\n",
       "         [-1.2890, -0.3704,  1.7607, -2.4346,  0.4207, -0.8050, -0.6024,\n",
       "          -1.6714, -1.7115, -1.2611],\n",
       "         [-1.0112,  0.0441, -0.2354, -1.0832, -0.8319, -1.3778,  0.7898,\n",
       "          -2.0225, -0.1850,  0.5123]],\n",
       "\n",
       "        [[ 0.3012, -0.6107,  0.1677, -1.1048,  0.5457, -0.1790,  1.0706,\n",
       "          -0.0209, -1.6347,  0.4812],\n",
       "         [ 1.4193,  0.2154, -2.6099, -1.9177,  0.2832,  0.7794,  0.0281,\n",
       "          -1.2503,  0.4508,  0.7400],\n",
       "         [ 1.4619,  0.9246,  1.9411,  1.2582,  2.2867, -0.1234,  0.1410,\n",
       "          -0.3073,  2.0698,  0.2418]],\n",
       "\n",
       "        [[ 1.1538, -0.0523, -0.6931,  0.5613,  1.0202,  2.2554, -0.6146,\n",
       "           0.3277,  0.6331,  0.6727],\n",
       "         [-0.1567, -0.5020, -1.8814,  1.0176, -0.7847,  0.7994,  0.7637,\n",
       "           0.3420, -0.7912, -0.2283],\n",
       "         [-1.4684,  0.1717, -1.5352, -0.8047, -0.6979, -0.1824, -0.1863,\n",
       "           0.0133,  1.5151, -1.4930]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "input = torch.randn(5, 3,10)\n",
    "input\n",
    "# batch, row, coloumn\n",
    "# batch, seq_length, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(10,20, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(10, 20, num_layers=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.randn(2,3,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8849, -0.0377, -0.8442, -0.3885, -0.3160, -0.9878, -1.7569,\n",
       "          -0.4813, -0.2370,  0.7164,  0.1766, -1.8464,  0.8618, -0.1484,\n",
       "          -0.9899,  0.8647, -0.6951, -0.6781, -1.3161,  0.2395],\n",
       "         [-0.0618,  0.8876, -1.8760,  0.1734,  2.0396,  2.7417,  0.9176,\n",
       "          -1.1632, -0.9284,  0.1284,  1.2373,  0.3357, -0.8940, -1.7979,\n",
       "           1.3789, -0.0955,  0.0741,  0.3966, -0.2160, -0.7145],\n",
       "         [-1.8246,  0.5814, -0.0167,  0.5478, -0.6746, -0.7669, -0.8837,\n",
       "          -0.1548, -1.3660, -0.3456, -0.0959,  1.5976,  0.4747,  0.5659,\n",
       "           0.2607, -2.0218, -1.3912, -1.2576, -1.2950,  0.8505]],\n",
       "\n",
       "        [[-0.3516,  0.0660, -2.3885,  1.8896,  1.3383, -0.1246, -0.2231,\n",
       "          -0.1445, -1.0238,  0.3231, -1.3092, -1.4012,  0.4618,  1.0453,\n",
       "          -1.0436, -0.4002, -1.0601, -0.4116,  0.9950, -0.2595],\n",
       "         [ 0.1968, -0.8300, -1.1702,  1.6192,  0.5013,  1.5713,  0.6121,\n",
       "          -0.2064, -0.3890,  1.7899, -0.1054,  1.0439, -0.9625,  0.5699,\n",
       "          -0.2515,  2.2055, -1.5631, -0.9100, -1.1250, -2.5316],\n",
       "         [ 1.0127,  0.6711,  1.7351, -1.3807, -0.1411,  0.1756, -0.0571,\n",
       "          -0.1044, -1.3446,  0.3619, -0.2803,  1.1759,  2.0124, -0.8875,\n",
       "           0.7514, -0.6475, -1.3153, -0.6039,  1.3863, -0.0489]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 20])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3358e-01,  2.0782e+00,  1.7463e+00, -1.1511e+00,  3.1073e-01,\n",
      "          7.0345e-01,  7.9059e-01,  4.5952e-01,  4.6525e-01, -5.1697e-01,\n",
      "         -1.8640e-01, -6.7928e-01, -1.2891e-02, -7.0636e-01, -3.6923e-01,\n",
      "         -1.6991e-01],\n",
      "        [ 8.4825e-01, -4.9683e-01, -1.0318e+00,  2.8291e-01, -4.5743e-01,\n",
      "         -6.9884e-01, -4.2754e-01, -1.7330e-01,  7.7686e-01, -5.4991e-01,\n",
      "         -1.7890e+00, -1.4420e+00, -1.6257e+00,  1.7252e+00,  1.3617e+00,\n",
      "         -6.7427e-01],\n",
      "        [ 2.2866e-01,  4.3696e-01,  5.1147e-01,  7.3411e-01,  6.0509e-01,\n",
      "         -7.3855e-01,  1.4007e-01, -1.3685e+00, -1.1359e+00,  4.9581e-01,\n",
      "          4.8565e-01,  7.1663e-01,  1.0555e-01,  2.1112e-01,  1.2970e+00,\n",
      "         -1.3649e+00],\n",
      "        [ 7.7281e-01,  2.3529e-01, -4.5390e-01,  1.1778e+00, -4.6243e-03,\n",
      "         -7.4004e-02, -5.5147e-02,  7.8496e-01, -5.4664e-01, -1.6033e+00,\n",
      "         -5.3937e-01, -1.3047e+00,  1.4342e+00, -3.0194e+00,  1.0339e+00,\n",
      "          1.2475e+00],\n",
      "        [-4.8489e-01,  2.8831e-01,  1.5676e-01, -1.6631e+00, -1.3953e+00,\n",
      "          1.4313e+00, -2.7399e+00,  1.0321e+00,  4.9113e-01,  3.2290e-01,\n",
      "         -1.5767e+00,  1.9328e-01, -2.6361e+00, -3.2978e-01,  7.4240e-01,\n",
      "         -1.0973e+00],\n",
      "        [ 1.1578e+00, -2.5193e-01,  8.1921e-01, -2.3759e-01, -7.5151e-01,\n",
      "         -8.4961e-01, -8.3914e-01, -1.0382e+00, -3.7926e-01,  1.1455e+00,\n",
      "          1.8654e+00, -9.9403e-01,  7.2730e-01,  2.3300e+00, -4.6433e-01,\n",
      "          1.4977e+00],\n",
      "        [-1.5617e+00, -1.2384e-01, -7.8325e-02,  9.9262e-01, -7.7843e-01,\n",
      "         -4.1182e-01,  1.0758e+00,  3.3776e-01,  1.7877e+00,  8.1822e-01,\n",
      "          2.3897e+00,  5.6522e-01,  5.7937e-01, -5.5723e-01, -1.9132e+00,\n",
      "         -1.9819e+00],\n",
      "        [-1.4864e+00,  1.4893e-01, -1.4335e+00, -5.6349e-01,  2.7042e-01,\n",
      "          3.4563e-01,  1.0261e+00, -7.7712e-01,  1.0769e-01,  7.7862e-01,\n",
      "          1.6257e+00, -6.3805e-01, -2.2171e-01,  1.0498e+00,  3.6977e-01,\n",
      "         -7.9991e-01],\n",
      "        [ 1.3687e+00, -8.3942e-01, -1.8360e-01, -6.6512e-02,  4.9280e-01,\n",
      "         -7.1314e-01, -3.3380e-01, -5.6056e-01, -1.9227e-01,  9.1755e-01,\n",
      "          3.4244e-01, -6.8334e-01,  7.0132e-01, -1.6725e+00, -8.4209e-01,\n",
      "          5.5615e-01],\n",
      "        [ 1.3791e+00,  1.7036e-01, -6.1586e-01,  4.4047e-01, -2.1198e-01,\n",
      "         -1.1657e+00, -9.7179e-01,  3.8662e-01,  4.2275e-03,  1.0015e+00,\n",
      "         -1.8809e+00,  2.6830e-01, -8.8623e-02, -4.5055e-01,  8.4344e-01,\n",
      "         -9.2942e-01],\n",
      "        [-3.3804e-01,  3.5552e-01, -2.7991e-01, -9.6621e-02,  1.0107e+00,\n",
      "          5.0369e-01,  1.2040e+00, -2.2790e-01,  1.6225e-01, -1.0010e+00,\n",
      "         -2.9290e+00, -1.1299e+00,  1.0841e+00, -1.5237e+00, -2.0625e-01,\n",
      "          1.3033e+00],\n",
      "        [ 3.8381e-01, -1.3759e-02, -1.3776e+00,  2.6024e-01, -1.8099e+00,\n",
      "         -1.3024e+00,  8.4221e-01,  1.3936e+00, -8.6812e-02,  3.2231e-01,\n",
      "         -1.0309e-01, -1.3197e+00,  5.8412e-01,  2.0581e+00,  1.4732e-01,\n",
      "         -1.4291e+00],\n",
      "        [ 2.6642e-01,  1.0095e+00,  2.5025e+00,  1.4234e-01, -7.3048e-01,\n",
      "         -5.0673e-01, -1.9910e-02,  9.1838e-01, -1.0947e-01,  3.0534e-01,\n",
      "          8.9260e-01,  1.2521e+00, -1.0105e-01, -1.1436e+00, -1.4685e+00,\n",
      "          4.3850e-01],\n",
      "        [-2.1069e-01,  3.6023e-01, -1.1783e-01,  4.1998e-02, -8.1032e-01,\n",
      "          3.0523e+00, -1.5050e+00,  6.2996e-01, -4.5895e-01, -9.0375e-01,\n",
      "          1.4883e+00, -1.0284e+00, -6.4646e-01, -2.6732e+00,  1.0816e+00,\n",
      "         -3.8877e-01],\n",
      "        [-5.1982e-01, -9.9812e-01,  2.1932e+00,  5.1099e-03,  1.1456e+00,\n",
      "          5.4119e-01, -4.2694e-01, -1.1546e+00, -1.2669e+00, -5.7272e-01,\n",
      "          6.2805e-01, -5.1555e-01, -2.4604e-01,  9.0217e-02, -2.6864e-01,\n",
      "          3.3721e-01],\n",
      "        [ 1.6319e-01,  3.4768e-01,  1.5411e+00,  8.1397e-01,  5.7605e-01,\n",
      "          3.6139e-01, -4.5054e-01, -4.1160e-01, -1.7793e-01,  4.0299e-01,\n",
      "          2.0487e-01,  1.0047e+00,  5.6775e-02, -6.3600e-01, -2.9308e-02,\n",
      "         -5.3844e-01],\n",
      "        [ 1.6540e+00,  1.1664e+00,  7.0349e-02, -3.0593e-01,  1.9397e+00,\n",
      "          1.9052e-01, -8.2917e-01,  1.6875e+00, -6.8404e-01, -1.1489e+00,\n",
      "         -1.6487e+00,  7.4785e-01, -2.7821e-02,  3.2761e-01, -1.2578e+00,\n",
      "         -1.2782e-01],\n",
      "        [ 1.0486e+00, -1.1470e+00,  1.7147e+00, -1.0167e+00,  1.3859e-01,\n",
      "          1.2516e+00, -1.5391e+00, -7.9601e-01,  1.5586e-01,  1.1363e+00,\n",
      "          1.3976e+00,  7.3669e-01, -6.3813e-01,  5.4510e-01, -1.3763e-01,\n",
      "          8.2407e-02],\n",
      "        [ 1.8736e-01, -3.6498e-01,  9.8758e-01, -6.0542e-01, -6.2536e-01,\n",
      "          5.2193e-01,  4.8775e-01, -5.6116e-01,  5.5619e-01,  2.1098e+00,\n",
      "          9.0794e-01, -8.4732e-01,  6.4077e-04,  4.0280e-01,  1.5405e+00,\n",
      "          5.7600e-02],\n",
      "        [-4.4769e-01, -2.3243e-01, -1.2410e+00,  7.5026e-01,  1.6973e+00,\n",
      "          8.0084e-01, -8.9413e-01, -3.0497e-01,  5.6990e-01,  8.6120e-01,\n",
      "         -6.4367e-01,  1.2204e+00,  1.2127e+00, -1.6237e-01,  1.4043e+00,\n",
      "         -7.0339e-01]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6698e-01,  2.5978e+00,  0.0000e+00, -1.4389e+00,  0.0000e+00,\n",
       "          8.7931e-01,  9.8823e-01,  5.7440e-01,  5.8157e-01, -6.4621e-01,\n",
       "         -2.3301e-01, -8.4909e-01, -1.6114e-02, -8.8295e-01, -4.6154e-01,\n",
       "         -2.1238e-01],\n",
       "        [ 1.0603e+00, -6.2104e-01, -0.0000e+00,  3.5364e-01, -5.7179e-01,\n",
       "         -0.0000e+00, -5.3443e-01, -2.1662e-01,  9.7108e-01, -6.8738e-01,\n",
       "         -2.2363e+00, -1.8025e+00, -2.0321e+00,  2.1565e+00,  0.0000e+00,\n",
       "         -8.4284e-01],\n",
       "        [ 2.8582e-01,  5.4620e-01,  6.3934e-01,  0.0000e+00,  0.0000e+00,\n",
       "         -9.2319e-01,  1.7509e-01, -0.0000e+00, -1.4198e+00,  6.1977e-01,\n",
       "          0.0000e+00,  0.0000e+00,  1.3194e-01,  0.0000e+00,  1.6212e+00,\n",
       "         -1.7062e+00],\n",
       "        [ 9.6602e-01,  2.9411e-01, -5.6737e-01,  1.4723e+00, -0.0000e+00,\n",
       "         -9.2505e-02, -6.8934e-02,  9.8120e-01, -0.0000e+00, -2.0041e+00,\n",
       "         -6.7422e-01, -1.6309e+00,  1.7928e+00, -0.0000e+00,  1.2923e+00,\n",
       "          1.5594e+00],\n",
       "        [-6.0612e-01,  3.6038e-01,  0.0000e+00, -2.0789e+00, -0.0000e+00,\n",
       "          0.0000e+00, -3.4248e+00,  1.2901e+00,  6.1392e-01,  4.0362e-01,\n",
       "         -1.9708e+00,  2.4159e-01, -3.2952e+00, -4.1222e-01,  9.2799e-01,\n",
       "         -0.0000e+00],\n",
       "        [ 0.0000e+00, -3.1491e-01,  1.0240e+00, -0.0000e+00, -9.3939e-01,\n",
       "         -1.0620e+00, -1.0489e+00, -1.2977e+00, -4.7407e-01,  1.4319e+00,\n",
       "          2.3318e+00, -1.2425e+00,  9.0913e-01,  2.9125e+00, -5.8041e-01,\n",
       "          1.8721e+00],\n",
       "        [-1.9521e+00, -1.5480e-01, -9.7907e-02,  1.2408e+00, -0.0000e+00,\n",
       "         -5.1478e-01,  1.3447e+00,  4.2220e-01,  2.2347e+00,  1.0228e+00,\n",
       "          2.9871e+00,  0.0000e+00,  7.2421e-01, -6.9654e-01, -0.0000e+00,\n",
       "         -0.0000e+00],\n",
       "        [-1.8580e+00,  1.8617e-01, -1.7919e+00, -0.0000e+00,  3.3802e-01,\n",
       "          4.3204e-01,  1.2826e+00, -0.0000e+00,  1.3461e-01,  0.0000e+00,\n",
       "          2.0321e+00, -7.9756e-01, -2.7714e-01,  0.0000e+00,  4.6221e-01,\n",
       "         -9.9989e-01],\n",
       "        [ 0.0000e+00, -1.0493e+00, -2.2950e-01, -0.0000e+00,  0.0000e+00,\n",
       "         -8.9143e-01, -4.1725e-01, -7.0070e-01, -2.4034e-01,  1.1469e+00,\n",
       "          4.2804e-01, -0.0000e+00,  8.7665e-01, -0.0000e+00, -1.0526e+00,\n",
       "          6.9518e-01],\n",
       "        [ 1.7239e+00,  2.1295e-01, -0.0000e+00,  5.5058e-01, -2.6498e-01,\n",
       "         -1.4571e+00, -1.2147e+00,  4.8327e-01,  0.0000e+00,  1.2519e+00,\n",
       "         -0.0000e+00,  3.3538e-01, -0.0000e+00, -5.6318e-01,  1.0543e+00,\n",
       "         -0.0000e+00],\n",
       "        [-4.2254e-01,  4.4440e-01, -3.4988e-01, -1.2078e-01,  1.2634e+00,\n",
       "          0.0000e+00,  1.5050e+00, -2.8487e-01,  2.0281e-01, -1.2512e+00,\n",
       "         -3.6612e+00, -0.0000e+00,  0.0000e+00, -1.9046e+00, -2.5781e-01,\n",
       "          1.6291e+00],\n",
       "        [ 0.0000e+00, -1.7199e-02, -1.7220e+00,  3.2530e-01, -0.0000e+00,\n",
       "         -1.6280e+00,  1.0528e+00,  1.7420e+00, -1.0851e-01,  4.0289e-01,\n",
       "         -1.2887e-01, -1.6496e+00,  7.3014e-01,  2.5727e+00,  1.8414e-01,\n",
       "         -1.7864e+00],\n",
       "        [ 3.3303e-01,  1.2618e+00,  3.1281e+00,  1.7792e-01, -9.1310e-01,\n",
       "         -6.3341e-01, -2.4887e-02,  1.1480e+00, -1.3684e-01,  3.8167e-01,\n",
       "          1.1157e+00,  1.5652e+00, -1.2632e-01, -1.4295e+00, -1.8356e+00,\n",
       "          5.4812e-01],\n",
       "        [-2.6336e-01,  4.5029e-01, -1.4728e-01,  5.2498e-02, -1.0129e+00,\n",
       "          0.0000e+00, -1.8813e+00,  7.8745e-01, -5.7368e-01, -1.1297e+00,\n",
       "          1.8604e+00, -1.2855e+00, -8.0807e-01, -3.3415e+00,  0.0000e+00,\n",
       "         -4.8596e-01],\n",
       "        [-6.4978e-01, -1.2476e+00,  0.0000e+00,  6.3873e-03,  0.0000e+00,\n",
       "          6.7649e-01, -5.3368e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
       "          7.8507e-01, -6.4444e-01, -3.0755e-01,  1.1277e-01, -3.3580e-01,\n",
       "          4.2151e-01],\n",
       "        [ 2.0399e-01,  4.3460e-01,  1.9264e+00,  1.0175e+00,  7.2006e-01,\n",
       "          4.5173e-01, -5.6318e-01, -5.1450e-01, -2.2241e-01,  5.0374e-01,\n",
       "          2.5609e-01,  1.2558e+00,  7.0968e-02, -7.9500e-01, -3.6635e-02,\n",
       "         -6.7305e-01],\n",
       "        [ 2.0675e+00,  1.4580e+00,  8.7937e-02, -0.0000e+00,  2.4246e+00,\n",
       "          2.3816e-01, -0.0000e+00,  2.1094e+00, -8.5505e-01, -1.4361e+00,\n",
       "         -2.0608e+00,  0.0000e+00, -3.4776e-02,  4.0951e-01, -1.5723e+00,\n",
       "         -1.5977e-01],\n",
       "        [ 1.3108e+00, -1.4338e+00,  2.1434e+00, -1.2709e+00,  1.7324e-01,\n",
       "          1.5645e+00, -1.9239e+00, -9.9501e-01,  1.9483e-01,  0.0000e+00,\n",
       "          1.7470e+00,  9.2086e-01, -7.9766e-01,  6.8137e-01, -1.7204e-01,\n",
       "          1.0301e-01],\n",
       "        [ 2.3420e-01, -4.5622e-01,  0.0000e+00, -7.5677e-01, -7.8171e-01,\n",
       "          0.0000e+00,  6.0968e-01, -0.0000e+00,  6.9524e-01,  2.6373e+00,\n",
       "          1.1349e+00, -1.0591e+00,  8.0096e-04,  5.0350e-01,  1.9256e+00,\n",
       "          7.1999e-02],\n",
       "        [-5.5962e-01, -2.9054e-01, -0.0000e+00,  0.0000e+00,  2.1216e+00,\n",
       "          1.0011e+00, -1.1177e+00, -0.0000e+00,  7.1237e-01,  0.0000e+00,\n",
       "         -8.0459e-01,  0.0000e+00,  1.5158e+00, -2.0297e-01,  1.7554e+00,\n",
       "         -8.7923e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Dropout(p=0.2)\n",
    "input = torch.randn(20, 16)\n",
    "print(input)\n",
    "output = m(input)\n",
    "output\n",
    "# The nn.Linear layer is a fully connected (dense) layer that maps the output of the LSTM to a probability distribution over the vocabulary. \n",
    "# This is a standard practice when predicting categorical values (in this case, words in a vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "            Forward pass of the encoder\n",
    "            Arguments:\n",
    "            - Features: Tensor of shape (batch_size, feature_size=512)\n",
    "            - caption: Tensor of shape (batch_size, max_caption_length), word indices\n",
    "            Returns:\n",
    "            - output: Tensor of shape (batch_size, max_caption_length, vocab_size), word prediction\n",
    "        \"\"\"\n",
    "        # Embedding the caption, excluding the <end> token\n",
    "        embedding = self.embedding(captions[:, :-1])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # Embedding layer: converts word indices into dense vectors of size embed_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # LSTM: input to hidden, hidden_size must match the size of features from CNN\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to map LSTM output to vocab_size\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Initialize the hidden state (if needed)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Optional dropout to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "        Arguments:\n",
    "        - features: Tensor of shape (batch_size, feature_size=512)\n",
    "        - captions: Tensor of shape (batch_size, max_caption_length), word indices\n",
    "        \n",
    "        Returns:\n",
    "        - outputs: Tensor of shape (batch_size, max_caption_length, vocab_size), word predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        # Embedding the captions, excluding the <end> token\"\n",
    "        embeddings = self.embedding(captions[:, :-1])\n",
    "        \n",
    "        # Concatenate the features with the embedded captions\n",
    "        # Features are passed as input to the first time step\n",
    "        features = features.unsqueeze(1)  # shape (batch_size, 1, feature_size)\n",
    "        lstm_input = torch.cat((features, embeddings), 1)  # shape (batch_size, 1 + caption_length, embed_size)\n",
    "        \n",
    "        # Pass the concatenated inputs through the LSTM\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        \n",
    "        # Pass the LSTM output through the fully connected layer to get word predictions\n",
    "        outputs = self.fc(lstm_out)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "from utils.vocab import Vocabulary\n",
    "\n",
    "vocab_file = '../vocab.pkl'\n",
    "\n",
    "# Build vocabulary (done earlier)\n",
    "if os.path.exists(vocab_file):\n",
    "    with open(vocab_file, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.60s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from models.encoder import EncoderCNN\n",
    "from models.decoder import DecoderRNN\n",
    "import torchvision.transforms as transforms\n",
    "from utils import data_loader \n",
    "from utils import vocab as vc\n",
    "from utils.vocab import Vocabulary\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "image_root = '../data'\n",
    "ann_file = '../data/captions_train2017.json'\n",
    "save_dir = '../checkpoints/'\n",
    "vocab_file = 'vocab.pkl'\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Build vocabulary (done earlier)\n",
    "if os.path.exists(vocab_file):\n",
    "    with open(vocab_file, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "else:\n",
    "    vocab = vc.build_vocab(ann_file, threshold=5)\n",
    "    with open(vocab_file, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab)  # Vocabulary size (make sure vocab.__len__() is implemented)\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "log_interval = 1  # Log every 10 batches\n",
    "batch_size = 32  # Batch size for training\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "subset_fraction = 0.001\n",
    "# Initialize models\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers=1).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Load data\n",
    "# Assume you have already created a DataLoader called data_loader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "data_loader = data_loader.get_loader(image_root, ann_file, vocab, transform, batch_size = 32, shuffle=True, num_workers=4, subset_fraction=subset_fraction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.8507, -0.5253, -0.1828,  ..., -0.9020, -0.7479, -1.2617],\n",
       "          [-0.9877, -0.6794, -0.3541,  ..., -0.4054, -0.3198, -1.0733],\n",
       "          [-1.0904, -0.9705, -0.5253,  ..., -0.8164, -0.8164, -1.0733],\n",
       "          ...,\n",
       "          [-1.1760, -1.1247, -1.0390,  ...,  0.9817,  0.6563,  0.3309],\n",
       "          [-1.1589, -1.0904, -1.0048,  ...,  0.7762,  0.3309,  0.0056],\n",
       "          [-1.1932, -1.1247, -0.9705,  ...,  0.6734,  0.3481,  0.0912]],\n",
       " \n",
       "         [[-1.3004, -1.0728, -0.8277,  ..., -0.7402, -0.5476, -1.0903],\n",
       "          [-1.4230, -1.2829, -0.8452,  ..., -0.1975, -0.1625, -0.8627],\n",
       "          [-1.5980, -1.4055, -0.9678,  ..., -0.5301, -0.6352, -0.9678],\n",
       "          ...,\n",
       "          [-0.6352, -0.6877, -0.6001,  ...,  0.4853,  0.1877, -0.0574],\n",
       "          [-0.7227, -0.6176, -0.5826,  ...,  0.3803, -0.0749, -0.4776],\n",
       "          [-0.8277, -0.7052, -0.5126,  ...,  0.3452,  0.0126, -0.4251]],\n",
       " \n",
       "         [[-1.2119, -0.9678, -0.7587,  ..., -0.2358, -0.1661, -0.6715],\n",
       "          [-1.3164, -1.0898, -0.8633,  ...,  0.1825,  0.1476, -0.4973],\n",
       "          [-1.4384, -1.2816, -1.0027,  ..., -0.0441, -0.1487, -0.4798],\n",
       "          ...,\n",
       "          [-0.3753, -0.3230, -0.2184,  ..., -0.6890, -1.0376, -1.2641],\n",
       "          [-0.4275, -0.2532, -0.1661,  ..., -0.8284, -1.1247, -1.3164],\n",
       "          [-0.4798, -0.3230, -0.1835,  ..., -0.9504, -1.1944, -1.3339]]]),\n",
       " tensor([  1,   4, 146,  36, 438,  40,  24, 235, 140,   2]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.batch_size\n",
    "(data_loader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.1657, -0.1486, -0.1486,  ..., -0.3027, -0.3027, -0.3541],\n",
      "          [-0.1314, -0.1314, -0.1314,  ..., -0.2856, -0.2856, -0.3369],\n",
      "          [-0.1486, -0.1314, -0.1314,  ..., -0.2856, -0.3027, -0.3198],\n",
      "          ...,\n",
      "          [-0.0116, -0.0972, -0.0972,  ..., -0.1999, -0.1657, -0.1828],\n",
      "          [ 0.0741,  0.0056,  0.1083,  ...,  0.0056,  0.0398, -0.1143],\n",
      "          [-0.0801,  0.0741,  0.1083,  ..., -0.1657, -0.1143, -0.0116]],\n",
      "\n",
      "         [[ 0.7479,  0.7654,  0.7654,  ...,  0.6429,  0.6078,  0.6078],\n",
      "          [ 0.7304,  0.7479,  0.7829,  ...,  0.6604,  0.6429,  0.6254],\n",
      "          [ 0.7654,  0.7829,  0.8004,  ...,  0.6604,  0.6604,  0.6078],\n",
      "          ...,\n",
      "          [ 0.1176,  0.0301,  0.0126,  ...,  0.0476,  0.1352,  0.1877],\n",
      "          [ 0.2052,  0.1352,  0.2227,  ...,  0.2752,  0.3277,  0.2402],\n",
      "          [ 0.0476,  0.2052,  0.2752,  ...,  0.1527,  0.1527,  0.2052]],\n",
      "\n",
      "         [[ 1.7511,  1.7685,  1.7685,  ...,  1.6291,  1.6117,  1.5768],\n",
      "          [ 1.7337,  1.7511,  1.7685,  ...,  1.6465,  1.6291,  1.5942],\n",
      "          [ 1.7511,  1.7685,  1.7860,  ...,  1.6640,  1.6291,  1.5942],\n",
      "          ...,\n",
      "          [ 0.2348,  0.1302,  0.1651,  ..., -0.2184, -0.1487, -0.1312],\n",
      "          [ 0.2696,  0.2522,  0.3393,  ..., -0.0615, -0.0092, -0.0790],\n",
      "          [ 0.1302,  0.3393,  0.3742,  ..., -0.1835, -0.0441, -0.0267]]],\n",
      "\n",
      "\n",
      "        [[[-0.9534, -0.9363, -0.9192,  ..., -0.7137, -0.7479, -0.6794],\n",
      "          [-0.9192, -0.9020, -0.8849,  ..., -0.6965, -0.7137, -0.6281],\n",
      "          [-0.8849, -0.8849, -0.8678,  ..., -0.6794, -0.6965, -0.5938],\n",
      "          ...,\n",
      "          [-1.0562, -1.0390, -1.0390,  ..., -0.5767, -0.5767, -0.5938],\n",
      "          [-1.0904, -1.0733, -1.0562,  ..., -0.6109, -0.6109, -0.5938],\n",
      "          [-1.0904, -1.0562, -1.0562,  ..., -0.6281, -0.6281, -0.6281]],\n",
      "\n",
      "         [[-1.1253, -1.1078, -1.0903,  ..., -0.8627, -0.9153, -0.8627],\n",
      "          [-1.0903, -1.0903, -1.0728,  ..., -0.8452, -0.8803, -0.8102],\n",
      "          [-1.0728, -1.0553, -1.0553,  ..., -0.8277, -0.8627, -0.7752],\n",
      "          ...,\n",
      "          [-1.2304, -1.2304, -1.2304,  ..., -0.7577, -0.7577, -0.7752],\n",
      "          [-1.2479, -1.2654, -1.2654,  ..., -0.7927, -0.7927, -0.7752],\n",
      "          [-1.2479, -1.2479, -1.2479,  ..., -0.8102, -0.8102, -0.8102]],\n",
      "\n",
      "         [[-1.1770, -1.1596, -1.1421,  ..., -0.9678, -1.0027, -0.9504],\n",
      "          [-1.1421, -1.1421, -1.1247,  ..., -0.9504, -0.9678, -0.8981],\n",
      "          [-1.1421, -1.1247, -1.1073,  ..., -0.9504, -0.9678, -0.8807],\n",
      "          ...,\n",
      "          [-1.3861, -1.4036, -1.3861,  ..., -0.8807, -0.8807, -0.9156],\n",
      "          [-1.4210, -1.4559, -1.4036,  ..., -0.9156, -0.8981, -0.9156],\n",
      "          [-1.4384, -1.4384, -1.4036,  ..., -0.9156, -0.9156, -0.9156]]],\n",
      "\n",
      "\n",
      "        [[[-0.5938, -0.6281, -0.5767,  ..., -0.6623, -0.6965, -0.7137],\n",
      "          [-0.6109, -0.7993, -0.7137,  ..., -0.6452, -0.6623, -0.6623],\n",
      "          [-0.6452, -0.7993, -0.7650,  ..., -0.5767, -0.6109, -0.6623],\n",
      "          ...,\n",
      "          [ 0.1083,  0.0227, -0.0801,  ...,  0.6563,  0.4166,  0.4337],\n",
      "          [ 0.1083,  0.0056, -0.1486,  ...,  0.6734,  0.5878,  0.4166],\n",
      "          [ 0.0912, -0.0116, -0.1999,  ...,  0.5364,  0.6563,  0.5364]],\n",
      "\n",
      "         [[-1.8606, -1.7031, -1.7731,  ..., -1.4055, -1.4755, -1.4755],\n",
      "          [-1.8256, -1.6331, -1.6331,  ..., -1.4055, -1.4405, -1.4405],\n",
      "          [-1.8606, -1.7031, -1.6681,  ..., -1.3880, -1.4055, -1.4405],\n",
      "          ...,\n",
      "          [-0.8627, -0.9503, -1.1078,  ..., -0.2500, -0.5126, -0.4776],\n",
      "          [-0.8277, -0.9503, -1.1429,  ..., -0.2325, -0.3200, -0.4951],\n",
      "          [-0.8452, -0.9503, -1.1779,  ..., -0.3901, -0.2500, -0.3725]],\n",
      "\n",
      "         [[-1.6999, -1.6127, -1.6302,  ..., -1.6650, -1.6650, -1.6476],\n",
      "          [-1.6824, -1.6302, -1.5953,  ..., -1.6824, -1.6302, -1.5953],\n",
      "          [-1.7347, -1.6824, -1.6476,  ..., -1.6476, -1.6476, -1.6476],\n",
      "          ...,\n",
      "          [-1.5430, -1.5779, -1.6476,  ..., -1.1073, -1.3513, -1.2990],\n",
      "          [-1.5430, -1.5953, -1.7173,  ..., -1.0898, -1.1596, -1.3164],\n",
      "          [-1.5256, -1.5779, -1.7347,  ..., -1.1944, -1.0550, -1.1770]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.3815, -1.3473, -1.4843,  ..., -2.0494, -2.0152, -2.0152],\n",
      "          [-1.3644, -1.2788, -1.3644,  ..., -2.0323, -2.0323, -2.0323],\n",
      "          [-1.4158, -1.3644, -1.3473,  ..., -2.0323, -2.0152, -2.0323],\n",
      "          ...,\n",
      "          [-1.8782, -1.8953, -1.8439,  ..., -1.7754, -1.8097, -1.7583],\n",
      "          [-1.8439, -1.8953, -1.8610,  ..., -1.7754, -1.7583, -1.7412],\n",
      "          [-1.7925, -1.8782, -1.8097,  ..., -1.7925, -1.7583, -1.7412]],\n",
      "\n",
      "         [[-0.3901, -0.4776, -0.3901,  ..., -1.8957, -1.8957, -1.8782],\n",
      "          [-0.3725, -0.4426, -0.3901,  ..., -1.8782, -1.9132, -1.8957],\n",
      "          [-0.3725, -0.3901, -0.3550,  ..., -1.8957, -1.8957, -1.8957],\n",
      "          ...,\n",
      "          [-1.8256, -1.8431, -1.8256,  ..., -1.7031, -1.7381, -1.7731],\n",
      "          [-1.8782, -1.8431, -1.8957,  ..., -1.7206, -1.7731, -1.7906],\n",
      "          [-1.8957, -1.8606, -1.8957,  ..., -1.7381, -1.7381, -1.7556]],\n",
      "\n",
      "         [[ 0.8274,  0.8971,  0.8622,  ..., -1.3687, -1.3513, -1.3687],\n",
      "          [ 0.8797,  0.9319,  0.9145,  ..., -1.4210, -1.4384, -1.4733],\n",
      "          [ 0.9494,  0.9319,  0.9319,  ..., -1.4384, -1.3513, -1.3687],\n",
      "          ...,\n",
      "          [-1.4733, -1.5779, -1.4733,  ..., -1.5081, -1.3861, -1.4210],\n",
      "          [-1.4907, -1.5779, -1.4907,  ..., -1.5081, -1.3513, -1.3861],\n",
      "          [-1.5256, -1.4907, -1.4559,  ..., -1.4036, -1.3513, -1.3861]]],\n",
      "\n",
      "\n",
      "        [[[-0.5253, -0.4397, -0.3712,  ..., -0.3712, -0.3712, -0.4054],\n",
      "          [-0.6109, -0.5424, -0.4739,  ..., -0.4397, -0.4054, -0.3883],\n",
      "          [-0.8335, -0.7650, -0.6794,  ..., -0.5082, -0.4226, -0.3541],\n",
      "          ...,\n",
      "          [-0.5082, -0.4739, -0.4568,  ..., -0.5253, -0.5767, -0.6109],\n",
      "          [-0.5253, -0.4911, -0.4739,  ..., -0.5596, -0.5938, -0.6281],\n",
      "          [-0.5596, -0.5082, -0.4911,  ..., -0.5767, -0.6109, -0.6452]],\n",
      "\n",
      "         [[-0.4776, -0.3901, -0.2675,  ..., -0.2675, -0.2850, -0.2850],\n",
      "          [-0.6001, -0.5301, -0.4426,  ..., -0.2850, -0.2675, -0.2325],\n",
      "          [-0.9153, -0.8803, -0.7577,  ..., -0.3025, -0.2500, -0.1975],\n",
      "          ...,\n",
      "          [-0.3550, -0.3200, -0.2850,  ..., -0.3725, -0.4076, -0.4426],\n",
      "          [-0.3550, -0.3375, -0.3025,  ..., -0.4076, -0.4251, -0.4601],\n",
      "          [-0.3725, -0.3550, -0.3200,  ..., -0.4076, -0.4426, -0.4776]],\n",
      "\n",
      "         [[-0.2707, -0.2184, -0.1138,  ..., -0.1661, -0.2010, -0.2184],\n",
      "          [-0.4450, -0.3578, -0.2532,  ..., -0.1312, -0.1138, -0.0964],\n",
      "          [-0.8807, -0.7936, -0.6715,  ..., -0.0441,  0.0082,  0.0605],\n",
      "          ...,\n",
      "          [-0.1487, -0.1138, -0.0790,  ..., -0.2707, -0.3055, -0.3578],\n",
      "          [-0.1661, -0.1312, -0.0964,  ..., -0.2881, -0.3230, -0.3753],\n",
      "          [-0.1835, -0.1661, -0.0964,  ..., -0.3055, -0.3404, -0.3753]]],\n",
      "\n",
      "\n",
      "        [[[-1.8782, -1.8782, -1.8953,  ..., -1.0219, -0.9192, -0.6965],\n",
      "          [-1.8782, -1.8953, -1.8953,  ..., -1.0390, -0.9020, -0.6794],\n",
      "          [-1.8782, -1.9124, -1.8953,  ..., -1.0390, -0.8849, -0.6623],\n",
      "          ...,\n",
      "          [-1.7925, -1.8097, -1.8268,  ..., -2.0837, -2.0837, -2.1008],\n",
      "          [-1.8439, -1.8610, -1.8782,  ..., -2.0837, -2.0837, -2.0837],\n",
      "          [-1.8610, -1.8953, -1.9124,  ..., -2.0837, -2.1008, -2.0837]],\n",
      "\n",
      "         [[-1.8431, -1.8431, -1.8782,  ..., -1.0903, -1.0378, -0.8102],\n",
      "          [-1.8431, -1.8606, -1.8782,  ..., -1.1253, -1.0203, -0.7927],\n",
      "          [-1.8431, -1.8782, -1.8782,  ..., -1.1253, -1.0028, -0.7577],\n",
      "          ...,\n",
      "          [-1.7731, -1.7906, -1.8081,  ..., -2.0007, -2.0007, -2.0182],\n",
      "          [-1.8081, -1.8081, -1.8081,  ..., -2.0007, -2.0007, -2.0007],\n",
      "          [-1.7906, -1.8256, -1.8606,  ..., -2.0007, -2.0182, -2.0007]],\n",
      "\n",
      "         [[-1.6999, -1.6999, -1.7173,  ..., -1.1421, -1.1247, -0.9678],\n",
      "          [-1.6999, -1.7173, -1.7173,  ..., -1.1596, -1.1073, -0.9504],\n",
      "          [-1.6999, -1.7347, -1.7173,  ..., -1.1596, -1.0898, -0.9330],\n",
      "          ...,\n",
      "          [-1.6302, -1.6476, -1.6476,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.6650, -1.6650, -1.6650,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.6476, -1.6824, -1.7173,  ..., -1.8044, -1.8044, -1.8044]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "        \n",
    "        # Move images and captions to the device (GPU or CPU)\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        print(images)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9534, -0.9363, -0.9192,  ..., -0.7137, -0.7479, -0.6794],\n",
       "         [-0.9192, -0.9020, -0.8849,  ..., -0.6965, -0.7137, -0.6281],\n",
       "         [-0.8849, -0.8849, -0.8678,  ..., -0.6794, -0.6965, -0.5938],\n",
       "         ...,\n",
       "         [-1.0562, -1.0390, -1.0390,  ..., -0.5767, -0.5767, -0.5938],\n",
       "         [-1.0904, -1.0733, -1.0562,  ..., -0.6109, -0.6109, -0.5938],\n",
       "         [-1.0904, -1.0562, -1.0562,  ..., -0.6281, -0.6281, -0.6281]],\n",
       "\n",
       "        [[-1.1253, -1.1078, -1.0903,  ..., -0.8627, -0.9153, -0.8627],\n",
       "         [-1.0903, -1.0903, -1.0728,  ..., -0.8452, -0.8803, -0.8102],\n",
       "         [-1.0728, -1.0553, -1.0553,  ..., -0.8277, -0.8627, -0.7752],\n",
       "         ...,\n",
       "         [-1.2304, -1.2304, -1.2304,  ..., -0.7577, -0.7577, -0.7752],\n",
       "         [-1.2479, -1.2654, -1.2654,  ..., -0.7927, -0.7927, -0.7752],\n",
       "         [-1.2479, -1.2479, -1.2479,  ..., -0.8102, -0.8102, -0.8102]],\n",
       "\n",
       "        [[-1.1770, -1.1596, -1.1421,  ..., -0.9678, -1.0027, -0.9504],\n",
       "         [-1.1421, -1.1421, -1.1247,  ..., -0.9504, -0.9678, -0.8981],\n",
       "         [-1.1421, -1.1247, -1.1073,  ..., -0.9504, -0.9678, -0.8807],\n",
       "         ...,\n",
       "         [-1.3861, -1.4036, -1.3861,  ..., -0.8807, -0.8807, -0.9156],\n",
       "         [-1.4210, -1.4559, -1.4036,  ..., -0.9156, -0.8981, -0.9156],\n",
       "         [-1.4384, -1.4384, -1.4036,  ..., -0.9156, -0.9156, -0.9156]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = encoder(images)\n",
    "features.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape\n",
    "# (batch size, each feature length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 23])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,    4,   61,  420,  218,    4, 2848,   36,   50,   10,  462,  124,\n",
       "          10,  109,    7,    4,   91,  151,   40,   10,  603,   13,    2],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "embed = nn.Embedding(vocab_size, 256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings = embed(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 23, 256])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6202e+00, -2.5387e+00, -1.3974e+00,  ...,  1.1184e+00,\n",
       "           1.0806e+00,  1.2296e+00],\n",
       "         [-8.6605e-01,  1.1939e-01,  2.2492e+00,  ..., -1.5899e+00,\n",
       "           3.4686e-01,  1.5243e+00],\n",
       "         [-1.9779e+00,  9.1244e-01,  6.3398e-01,  ..., -3.3810e-01,\n",
       "           3.2364e-01, -1.0827e-02],\n",
       "         ...,\n",
       "         [ 4.2300e-01, -4.2027e-01, -2.7811e-01,  ...,  1.0645e+00,\n",
       "          -7.4412e-01,  3.9002e-01],\n",
       "         [-1.3493e+00,  1.4673e+00, -5.9974e-01,  ..., -5.4825e-01,\n",
       "          -1.3084e+00,  2.9896e-01],\n",
       "         [-1.9171e-03, -9.1087e-02, -3.4613e-01,  ..., -1.0111e-01,\n",
       "          -1.6845e-01, -8.4402e-01]],\n",
       "\n",
       "        [[ 1.6202e+00, -2.5387e+00, -1.3974e+00,  ...,  1.1184e+00,\n",
       "           1.0806e+00,  1.2296e+00],\n",
       "         [ 6.5716e-01,  3.0918e-01, -4.7231e-01,  ...,  1.2112e+00,\n",
       "          -3.3847e-01,  2.1340e+00],\n",
       "         [ 4.6459e-01, -1.7325e-01,  1.9442e+00,  ...,  2.1383e+00,\n",
       "           9.9002e-02, -1.6520e-01],\n",
       "         ...,\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01],\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01],\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01]],\n",
       "\n",
       "        [[ 1.6202e+00, -2.5387e+00, -1.3974e+00,  ...,  1.1184e+00,\n",
       "           1.0806e+00,  1.2296e+00],\n",
       "         [-8.6605e-01,  1.1939e-01,  2.2492e+00,  ..., -1.5899e+00,\n",
       "           3.4686e-01,  1.5243e+00],\n",
       "         [ 2.6971e-01,  1.2934e+00,  2.8168e-01,  ...,  3.7501e-01,\n",
       "           3.8450e-01, -8.4064e-01],\n",
       "         ...,\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01],\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01],\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.6202e+00, -2.5387e+00, -1.3974e+00,  ...,  1.1184e+00,\n",
       "           1.0806e+00,  1.2296e+00],\n",
       "         [-1.0956e+00, -1.7189e+00,  8.8942e-01,  ..., -1.5999e+00,\n",
       "          -1.5487e+00, -5.6174e-01],\n",
       "         [ 4.6459e-01, -1.7325e-01,  1.9442e+00,  ...,  2.1383e+00,\n",
       "           9.9002e-02, -1.6520e-01],\n",
       "         ...,\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01],\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01],\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01]],\n",
       "\n",
       "        [[ 1.6202e+00, -2.5387e+00, -1.3974e+00,  ...,  1.1184e+00,\n",
       "           1.0806e+00,  1.2296e+00],\n",
       "         [-1.5426e+00, -2.9753e-01,  9.8250e-01,  ..., -4.8059e-01,\n",
       "          -1.2618e+00, -1.8860e+00],\n",
       "         [ 4.6459e-01, -1.7325e-01,  1.9442e+00,  ...,  2.1383e+00,\n",
       "           9.9002e-02, -1.6520e-01],\n",
       "         ...,\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01],\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01],\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01]],\n",
       "\n",
       "        [[ 1.6202e+00, -2.5387e+00, -1.3974e+00,  ...,  1.1184e+00,\n",
       "           1.0806e+00,  1.2296e+00],\n",
       "         [-8.6605e-01,  1.1939e-01,  2.2492e+00,  ..., -1.5899e+00,\n",
       "           3.4686e-01,  1.5243e+00],\n",
       "         [-1.0080e-01, -3.1805e-01, -4.4756e-01,  ...,  1.2674e+00,\n",
       "           4.3414e-01,  7.5352e-01],\n",
       "         ...,\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01],\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01],\n",
       "         [-1.3263e+00,  7.3410e-01,  1.0591e+00,  ..., -7.7738e-01,\n",
       "           1.2264e-01, -3.6624e-01]]], device='cuda:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6202e+00, -2.5387e+00, -1.3974e+00,  ...,  1.1184e+00,\n",
       "          1.0806e+00,  1.2296e+00],\n",
       "        [-8.6605e-01,  1.1939e-01,  2.2492e+00,  ..., -1.5899e+00,\n",
       "          3.4686e-01,  1.5243e+00],\n",
       "        [-1.9779e+00,  9.1244e-01,  6.3398e-01,  ..., -3.3810e-01,\n",
       "          3.2364e-01, -1.0827e-02],\n",
       "        ...,\n",
       "        [ 4.2300e-01, -4.2027e-01, -2.7811e-01,  ...,  1.0645e+00,\n",
       "         -7.4412e-01,  3.9002e-01],\n",
       "        [-1.3493e+00,  1.4673e+00, -5.9974e-01,  ..., -5.4825e-01,\n",
       "         -1.3084e+00,  2.9896e-01],\n",
       "        [-1.9171e-03, -9.1087e-02, -3.4613e-01,  ..., -1.0111e-01,\n",
       "         -1.6845e-01, -8.4402e-01]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(256, 512, 1, batch_first=True)\n",
    "fc = nn.Linear(hidden_size, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 23, 256])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7884e-01,  3.5485e-02,  1.8867e-01,  2.1133e-01,  3.0049e-01,\n",
       "        -1.8708e-01,  1.0471e-02, -2.4576e-01,  1.5605e-01,  4.5011e-01,\n",
       "        -9.1738e-02, -8.0126e-02, -2.6264e-01, -8.8581e-02, -1.5897e-01,\n",
       "         3.4028e-01,  1.3875e-01, -7.2856e-02, -1.3199e-01,  1.7333e-01,\n",
       "        -2.1683e-01,  3.1568e-02,  1.2569e-01, -3.8034e-01, -2.3918e-01,\n",
       "        -9.8834e-02, -1.4781e-01,  1.8908e-01,  2.1438e-02, -3.7536e-02,\n",
       "         7.2187e-02, -8.9411e-02, -1.2113e-03, -1.2203e-01,  5.9581e-02,\n",
       "        -2.2986e-02,  8.0658e-02, -3.3516e-01, -1.8464e-01,  3.7603e-01,\n",
       "         7.3006e-02, -1.9758e-01, -3.8929e-02,  3.1313e-03, -2.1165e-02,\n",
       "         8.6840e-02,  3.9826e-01, -1.1354e-01, -1.0715e-02,  2.3976e-01,\n",
       "         2.1704e-02, -9.9393e-02, -1.3941e-01, -1.7334e-01,  4.3932e-01,\n",
       "        -1.2830e-01, -1.9109e-01, -7.7809e-03,  1.5201e-01, -1.5800e-01,\n",
       "         1.1645e-01, -2.0851e-02, -3.4033e-01, -1.0205e-01, -7.5210e-02,\n",
       "        -9.5101e-02, -4.6465e-02,  6.5427e-01, -9.2827e-02, -3.2036e-02,\n",
       "        -1.4316e-01,  6.0788e-02,  2.3750e-02,  7.3194e-02, -2.7707e-01,\n",
       "        -9.2598e-02, -2.2020e-01,  1.5619e-01, -5.1213e-02,  3.3306e-01,\n",
       "         6.7350e-03, -1.8094e-01, -1.6010e-01,  1.4300e-01,  1.5226e-01,\n",
       "        -1.0210e-01,  2.5520e-04, -1.8744e-01, -2.0010e-01, -7.6947e-02,\n",
       "        -1.4581e-02, -1.9402e-01, -7.0206e-02,  1.9937e-02, -1.2975e-01,\n",
       "         2.5694e-02, -3.5062e-01,  1.0340e-02,  1.5349e-02,  9.8653e-03,\n",
       "         1.9766e-01,  3.7806e-02,  3.0880e-01,  2.1747e-02, -4.9477e-02,\n",
       "        -3.2741e-01, -1.1543e-01, -1.9832e-01,  1.2986e-01,  3.0809e-01,\n",
       "        -1.0325e-01, -3.5662e-01,  1.5271e-01, -1.1707e-01, -3.4988e-01,\n",
       "        -3.2557e-02, -2.2207e-01,  2.3440e-01, -9.4104e-02, -2.3909e-01,\n",
       "         8.6538e-02,  1.2664e-01,  2.5664e-01, -1.5152e-02,  4.9794e-02,\n",
       "         3.0983e-01, -2.4957e-01,  4.8202e-02,  9.9171e-02,  1.0659e-01,\n",
       "         3.0952e-02,  3.9320e-01,  1.6602e-01,  1.0440e-01,  3.0539e-01,\n",
       "         9.2814e-02,  1.1724e-01, -1.2891e-01, -2.0324e-01, -6.2905e-02,\n",
       "        -1.4228e-01, -2.3022e-01, -2.3306e-01, -8.9323e-02, -8.3414e-02,\n",
       "         3.6119e-02,  5.1068e-02, -2.8179e-02, -1.3418e-01, -1.3413e-01,\n",
       "        -8.7052e-02, -7.9151e-02,  9.5687e-02, -1.7373e-01,  1.2958e-02,\n",
       "        -4.9230e-01, -1.0149e-01, -1.6027e-01, -3.0982e-01,  7.8786e-02,\n",
       "         1.6194e-01,  1.2103e-01,  1.0624e-01, -6.7318e-02, -6.4931e-02,\n",
       "         1.7179e-02, -9.4592e-02, -2.8029e-02,  3.1715e-01, -1.6218e-01,\n",
       "        -1.7331e-01, -2.9233e-02,  1.4371e-01, -5.6653e-02, -1.5330e-01,\n",
       "         5.7735e-01,  6.9325e-02, -1.9113e-01,  1.1888e-01, -2.7369e-01,\n",
       "         1.2537e-01, -3.5400e-03,  7.6514e-02, -7.7822e-02,  2.1848e-01,\n",
       "        -1.8250e-02, -1.8768e-01, -2.0417e-01, -3.4640e-02, -2.5778e-01,\n",
       "        -1.3790e-01, -5.3585e-02, -1.3336e-01,  1.2636e-01,  2.0922e-01,\n",
       "         3.2530e-01, -3.6410e-01, -3.9120e-02,  8.5466e-02,  1.0857e-01,\n",
       "        -1.3319e-01,  2.0545e-01, -1.5811e-01,  3.4058e-01, -2.0396e-01,\n",
       "        -2.7263e-02,  2.4991e-01, -1.5233e-01,  2.1518e-02, -1.5529e-01,\n",
       "        -8.1236e-02, -9.4935e-02,  4.3330e-03, -3.5243e-01,  3.3490e-02,\n",
       "        -1.0375e-01, -4.9715e-01,  5.6403e-02, -2.8936e-01,  4.7977e-02,\n",
       "        -3.0731e-01, -6.3088e-02, -1.6573e-01, -2.1683e-01, -1.2590e-01,\n",
       "         9.2307e-02, -1.1535e-01, -3.4097e-01, -3.4955e-01,  7.7092e-02,\n",
       "        -4.5812e-02, -3.9547e-02, -3.0594e-02,  3.4474e-01, -1.0570e-02,\n",
       "         1.1787e-01,  3.4830e-02, -2.4699e-01,  3.3626e-02,  3.7685e-01,\n",
       "        -1.2733e-01,  1.4188e-01, -2.5920e-02,  1.2180e-01,  2.1546e-01,\n",
       "         2.1384e-02, -1.8043e-01, -1.7053e-01,  3.4694e-01,  2.2405e-01,\n",
       "         2.3590e-01,  2.2307e-01, -1.2381e-01,  9.4471e-02,  1.2232e-01,\n",
       "         1.2593e-02], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 256])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = features.unsqueeze(1)\n",
    "features.shape\n",
    "# to concanate with embeding of size(bath size, seq_length, embeding size)\n",
    "# we transpose or conver to 1 row and 256 coloumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = torch.cat((features, embedings), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 24, 256])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_input.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(256, 512, batch_first=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs: output, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out, (hn, cn) = lstm(lstm_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 24, 512])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out = lstm_out[:,:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=9439, bias=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = fc(lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 23, 9439])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9439"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted words for sequence 0: ['bathed', 'suits', 'piano', 'travelers', 'regarding', 'pouch', 'atv', 'aircraft', 'california', 'demonic', 'demonic', 'rains', 'pops', 'merchant', 'possessions', 'possessions', 'aircraft', 'protection', 'stay', 'sneaker', 'adult', 'keychain', 'thai']\n",
      "Predicted words for sequence 1: ['evil', 'suits', 'juicy', 'gazes', 'texas', 'dig', 'arabic', 'arabic', 'recorder', 'according', 'mama', 'investigates', 'plunger', 'funky', 'toppings', 'working', 'flames', 'smashed', 'yankees', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 2: ['aircraft', 'suits', 'piano', 'theater', 'apparatus', 'plunger', 'mph', 'beat', 'ground', 'atv', 'once', 'cameraman', 'california', 'draped', 'den', 'fingertips', 'av', 'yankees', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 3: ['pedestrians', 'suits', 'piano', 'crumbs', 'roasted', 'mugs', 'manmade', 'once', 'pressing', 'sheets', 'california', 'fenced', 'motorcycles', 'bot', 'matching', 'av', 'yankees', 'directing', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 4: ['monitoring', 'suits', 'cycling', 'tail', 'evil', 'cameraman', 'filth', 'ear', 'sponsored', 'providing', 'got', 'antennae', 'extends', 'california', 'bakers', 'motorists', '!', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 5: ['air', 'suits', 'piano', 'strokes', 'above', 'props', 'brave', 'beat', 'slope', 'wool', 'tin', 'moored', 'tennis', 'pulled', 'av', 'yankees', 'directing', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 6: ['fuel', 'suits', 'piano', 'clementine', 'him', 'sponsored', 'charge', 'soiled', 'him', 'sponsored', 'nike', 'california', 'becomes', 'belly', 'subject', 'normal', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 7: ['out', 'suits', 'california', 'theater', 'apparatus', 'dig', 'cents', 'occupy', 'humming', 'providing', 'crouched', 'slope', 'wool', 'wool', 'wrench', 'yankees', 'recognizable', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 8: ['out', 'suits', 'piano', 'crumbs', 'alertly', 'atv', 'cellar', 'sponsored', 'disarray', 'pharmacy', 'pharmacy', 'apple', 'lapel', 'comforters', 'av', 'yankees', 'directing', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 9: ['motorcycles', 'suits', 'piano', 'comforters', 'slope', 'ground', 'closest', 'link', 'charge', 'browns', 'browns', 'browns', 'modem', 'arranged', 'yankees', 'directing', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 10: ['pedestrians', 'suits', 'piano', 'collect', 'gazes', 'grade', 'seasonings', 'draped', 'aircraft', 'aircraft', 'wallpapered', 'trail', 'ram', 'smashed', 'trail', 'handstand', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 11: ['pedestrians', 'suits', 'piano', 'embracing', 'smack', 'wool', 'california', 'bot', 'mph', 'crumbs', 'piano', 'casts', 'stockings', 'contents', 'crucifix', 'para-sail', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 12: ['motorhome', 'suits', 'piano', 'surfboards', 'peels', 'whle', 'motorcycles', 'aircraft', 'chalk', 'shoulder', 'mph', 'facing', 'messily', 'motorists', '!', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 13: ['pulled', 'suits', 'piano', 'crumbs', 'piano', 'occupants', 'blurs', 'filled', 'charge', 'shuttered', 'scissor', 'bot', 'greets', 'motorists', 'directing', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 14: ['pedestrians', 'suits', 'piano', 'giraffee', 'gazes', 'ran', 'parasailers', 'seasonings', 'flanking', 'winds', 'finger', 'finger', 'liter', 'yankees', 'directing', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 15: ['catamaran', 'suits', 'piano', 'projector', 'touches', 'tour', 'charge', 'piano', 'sons', 'trellis', 'dishwasher', 'overstuffed', 'ramp', 'yankees', 'directing', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 16: ['hangs', 'suits', 'piano', 'stripes', 'surprised', 'wigs', 'after', 'theater', 'outcropping', 'sparkling', 'tennis', 'eight', 'save', 'yankees', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 17: ['pedestrians', 'suits', 'dishwasher', 'playstation', 'approval', 'bake', 'plunger', 'moored', 'side', 'raquet', 'six', 'av', 'ran', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 18: ['pedestrians', 'suits', 'drawers', 'asking', 'california', 'harbour', 'plunger', 'piggy', 'passing', 'kitchenware', 'browns', 'shoulder', 'knick-knacks', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 19: ['pinning', 'suits', 'beret', 'hall', 'heads', 'tear', 'providing', 'crush', 'leafed', 'drooping', 'brave', 'smashed', 'yankees', '!', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 20: ['relish', 'terrace', 'piano', 'bedding', 'juicy', 'rabbits', 'ear', 'sun', 'mosquito', 'california', 'kitchenware', 'toronto', 'ran', 'fired', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 21: ['slanted', 'suits', 'ran', 'brave', 'strewn', 'circle', 'halloween', 'informal', 'flanking', 'apples', 'stockings', 'old-fashioned', 'crucifix', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 22: ['signing', 'suits', 'piano', 'cops', 'heads', 'insect', 'booty', 'nike', 'smokes', 'crop', 'trunks', 'cafe', 'yankees', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 23: ['outcropping', 'suits', 'piano', 'fanning', 'triangles', 'contraption', 'adolescent', 'plunger', 'california', 'alertly', 'canister', 'canister', 'yankees', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 24: ['urinals', 'suits', 'piano', 'enough', 'canister', 'reporters', 'describing', 'hydrogen', 'banks', 'stockings', 'items', 'crucifix', 'directing', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 25: ['outcropping', 'suits', 'piano', 'crumbs', 'california', 'repairs', 'cameraman', 'airports', 'aircraft', 'piano', 'toronto', 'bush', '!', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 26: ['slanted', 'suits', 'drawers', 'asking', 'patchwork', 'young', 'rabbits', 'investigates', 'contrasting', 'cylinders', 'film', 'contemplating', 'recognizable', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 27: ['corks', 'suits', 'piano', 'comparison', 'wool', 'ran', 'cultural', 'cultural', 'shorter', 'early', 'moldy', 'ran', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 28: ['advancing', 'suits', 'piano', 'towl', 'extra', 'whip', 'peppers', 'smokes', 'oceans', 'daisies', 'restaraunt', 'motorists', 'directing', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 29: ['catamaran', 'suits', 'nibbles', 'slope', 'sectional', 'outline', 'apple', 'atv', 'drags', 'contentedly', 'bottled', 'motorists', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 30: ['hangs', 'suits', 'globe', 'frontier', 'actively', 'exiting', 'kfc', 'pet', 'charge', 'idea', 'air', 'yankees', 'rockaway', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n",
      "Predicted words for sequence 31: ['fisherman', 'suits', 'piano', 'theater', 'prepare', 'furnace', 'inside', 'belly', 'well-organized', 'perches', 'dollhouse', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie', 'walkie']\n"
     ]
    }
   ],
   "source": [
    "# Assuming output has the shape (batch_size, seq_len, vocab_size)\n",
    "batch_size, seq_len, vocab_size = output.shape\n",
    "\n",
    "# Get the predicted word indices for each time step across the sequence\n",
    "_, predicted_indices = torch.max(output, dim=2)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "# Convert the predicted indices to actual words using your vocabulary\n",
    "predicted_words = []\n",
    "for i in range(batch_size):\n",
    "    words = [vocab.idx2word[idx.item()] for idx in predicted_indices[i]]\n",
    "    predicted_words.append(words)\n",
    "\n",
    "# Print predicted words for each sequence in the batch\n",
    "for i, words in enumerate(predicted_words):\n",
    "    print(f\"Predicted words for sequence {i}: {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 23])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4352, 3282, 4030, 2461, 4052, 1930,  799, 1095, 3366, 7332, 7332,  878,\n",
       "        4697, 5388, 4569, 4569, 1095, 7588, 1792, 7459,  913, 3664, 2819],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 23, 512])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 23, 9439])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.view(-1, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([736, 9439])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted_indices = torch.max(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([736])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4352, device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 23])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = captions[:, :].contiguous().view(-1)  # Shape: (batch_size * sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([736])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([736, 9439])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([736])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([736])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.to(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_indices-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([736])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "736"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: ['<start>'], predicted words: ['bathed']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(targets)):\n",
    "            predicted_words = [vocab.idx2word[predicted_indices[i].item()] ]\n",
    "            target_words = [vocab.idx2word[targets[i].item()]]\n",
    "            print(f\"target: {target_words}, predicted words: {predicted_words}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
