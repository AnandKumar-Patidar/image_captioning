{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input weights W_{hx}:\n",
      " Parameter containing:\n",
      "tensor([[-0.4996,  0.3494,  0.2941,  0.4295],\n",
      "        [ 0.5082,  0.3466,  0.3223, -0.5622],\n",
      "        [ 0.2575,  0.0025,  0.1807,  0.1331]], requires_grad=True)\n",
      "Hidden weights W_{hh}:\n",
      " Parameter containing:\n",
      "tensor([[-0.4459, -0.3941,  0.5222],\n",
      "        [-0.2701,  0.0718,  0.5033],\n",
      "        [-0.4235,  0.1081,  0.4351]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create an RNN\n",
    "rnn = nn.RNN(input_size=4, hidden_size=3, num_layers=1)\n",
    "\n",
    "# Check initial weights\n",
    "print(\"Input weights W_{hx}:\\n\", rnn.weight_ih_l0)  # Weights for input to hidden\n",
    "print(\"Hidden weights W_{hh}:\\n\", rnn.weight_hh_l0)  # Weights for hidden to hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "('weight_ih_l0', Parameter containing:\n",
      "tensor([[-0.4996,  0.3494,  0.2941,  0.4295],\n",
      "        [ 0.5082,  0.3466,  0.3223, -0.5622],\n",
      "        [ 0.2575,  0.0025,  0.1807,  0.1331]], requires_grad=True))\n",
      "('weight_hh_l0', Parameter containing:\n",
      "tensor([[-0.4459, -0.3941,  0.5222],\n",
      "        [-0.2701,  0.0718,  0.5033],\n",
      "        [-0.4235,  0.1081,  0.4351]], requires_grad=True))\n",
      "('bias_ih_l0', Parameter containing:\n",
      "tensor([-0.4059,  0.1711,  0.1784], requires_grad=True))\n",
      "('bias_hh_l0', Parameter containing:\n",
      "tensor([-0.2112, -0.3734,  0.5158], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for i in rnn.named_parameters():\n",
    "    print(type(i))\n",
    "for i in rnn.named_parameters():\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.4996,  0.3494,  0.2941,  0.4295],\n",
      "        [ 0.5082,  0.3466,  0.3223, -0.5622],\n",
      "        [ 0.2575,  0.0025,  0.1807,  0.1331]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in rnn.named_parameters():\n",
    "    print(i[0], i[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2148,  0.8306,  0.4785, -0.2067],\n",
      "        [ 0.3863,  0.7520, -0.5327, -0.3851],\n",
      "        [-0.0748,  0.2377, -0.2322, -0.5978]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.9204,  0.7638,  0.4592],\n",
      "        [-0.2544, -0.9840,  0.4678],\n",
      "        [-0.7433,  0.2672, -0.8830]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize weights with Xavier uniform distribution\n",
    "for name, param in rnn.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        nn.init.xavier_uniform_(param)\n",
    "        print(param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Initializing Weights with a Uniform Distribution\n",
    "\n",
    "Initializing weights using a **uniform distribution** offers several important benefits, particularly in the context of neural networks. Let’s break down the main advantages:\n",
    "\n",
    "#### 1. Breaking Symmetry\n",
    "\n",
    "One of the primary reasons for initializing weights with a uniform distribution (or any random distribution) is to **break symmetry** between the neurons. If all weights are initialized to the same value (e.g., zeros), each neuron in a layer will receive the same gradients and update in the same way during training, effectively making them learn the same features. Random initialization with a uniform distribution ensures that neurons start with different weights, allowing them to learn different aspects of the data.\n",
    "\n",
    "- **Benefit**: Different neurons can learn different features, leading to a more expressive and powerful network.\n",
    "\n",
    "#### 2. Ensuring Appropriate Scale of Weights\n",
    "\n",
    "A uniform distribution allows control over the range of the initial weights, which can help keep the initial activations and gradients at a manageable scale, preventing problems like vanishing or exploding gradients. Proper initialization, particularly with small random values, helps ensure that the input signals neither shrink nor grow too much as they propagate through the network.\n",
    "\n",
    "- **Benefit**: Reduces the risk of vanishing or exploding gradients, leading to more stable and efficient training.\n",
    "\n",
    "#### 3. Faster Convergence\n",
    "\n",
    "Random weight initialization using a uniform distribution, especially when coupled with techniques like **Xavier/Glorot** or **He/Kaiming** initialization, helps the network converge faster by providing weights that are well-scaled for the specific activation functions in use (e.g., sigmoid, ReLU). These initialization techniques are designed to keep the variance of the outputs consistent across layers, which can significantly improve the training speed and convergence.\n",
    "\n",
    "- **Benefit**: Improved training speed and faster convergence to an optimal solution.\n",
    "\n",
    "#### 4. Flexibility\n",
    "\n",
    "Using a uniform distribution offers flexibility in controlling the range of initial weights. By specifying the bounds (e.g., `[-a, a]`), you can ensure the initial weights are not too large or too small, which can help avoid large fluctuations or vanishing signals during forward and backward propagation.\n",
    "\n",
    "- **Benefit**: Control over the range of initial weights, reducing the risk of extreme initial values that can destabilize training.\n",
    "\n",
    "#### 5. Good for Large Networks\n",
    "\n",
    "Uniform initialization works well in practice for large networks, as it ensures that each neuron starts with a different weight but within a controlled range. This is especially important in deep neural networks, where improper weight initialization can cause problems as the signals pass through many layers.\n",
    "\n",
    "- **Benefit**: Uniform initialization provides a practical and scalable solution for initializing weights in large, deep neural networks.\n",
    "\n",
    "### Common Methods Based on Uniform Distribution\n",
    "\n",
    "Several commonly used weight initialization techniques are based on uniform distributions. These methods adjust the range of the uniform distribution to account for the size of the input and output layers:\n",
    "\n",
    "1. **Xavier/Glorot Uniform Initialization**:\n",
    "   - Uses a uniform distribution with the range dependent on the number of input and output neurons.\n",
    "   - Designed for use with sigmoid and tanh activation functions.\n",
    "   \n",
    "      \\[\n",
    "      $W \\sim \\mathcal{U}\\left( -\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}} \\right)\n",
    "      \\]\n",
    "      where \\( n \\) is the number of input and output units in the layer.\n",
    "\n",
    "2. **He/Kaiming Uniform Initialization**:\n",
    "   - Uses a uniform distribution with the range adjusted for ReLU and variants.\n",
    "   \n",
    "   \\[\n",
    "   W \\sim \\mathcal{U}\\left( -\\sqrt{\\frac{6}{n}}, \\sqrt{\\frac{6}{n}} \\right)\n",
    "   \\]\n",
    "   where \\( n \\) is the number of input units in the layer.\n",
    "\n",
    "These methods ensure that the variance of the activations remains consistent across layers, which is critical for efficient training.\n",
    "\n",
    "### Summary of Benefits\n",
    "\n",
    "1. **Breaking Symmetry**: Ensures neurons have unique weights, allowing them to learn different features.\n",
    "2. **Appropriate Weight Scale**: Helps avoid vanishing or exploding gradients by controlling the range of initial weights.\n",
    "3. **Faster Convergence**: Properly initialized weights lead to faster and more stable convergence during training.\n",
    "4. **Flexibility**: You can control the bounds of the uniform distribution to fit the network’s needs.\n",
    "5. **Scalable to Large Networks**: Uniform initialization is practical for large, deep networks and can be tailored for different activation functions.\n",
    "\n",
    "In conclusion, initializing weights with a uniform distribution ensures that the neural network can learn efficiently from the start, prevents various common issues during training, and improves convergence speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1499, -0.3657, -0.7299, -0.1267, -0.7077, -0.2641, -2.6540,\n",
       "          -0.1504, -1.1138,  0.5935],\n",
       "         [ 0.4498, -0.1167,  1.1558,  1.1256, -0.2923,  1.4538,  0.1645,\n",
       "          -0.7498,  1.7855,  0.7669],\n",
       "         [-1.0417,  0.5994, -0.7215,  0.8563,  1.1804, -0.5962,  0.1644,\n",
       "           1.0475, -0.2483, -0.2710]],\n",
       "\n",
       "        [[-0.5920,  1.2169,  0.9833,  0.1513,  0.2068, -1.1294, -1.5255,\n",
       "          -1.4176,  0.9134, -1.3590],\n",
       "         [ 0.3466,  0.7718, -0.1248,  0.4561, -0.9916,  0.2479, -1.5459,\n",
       "          -0.3620, -0.1774, -0.7092],\n",
       "         [-0.5474, -0.7459, -0.4226, -0.5895,  0.0923, -0.4525,  0.3769,\n",
       "           1.0127, -0.1297,  0.3434]],\n",
       "\n",
       "        [[ 0.9408, -0.5802,  0.5878,  0.4247, -0.0800, -0.3107,  0.2853,\n",
       "          -0.2015, -0.3247, -2.3850],\n",
       "         [-0.1829,  1.1241,  0.5454, -0.7379, -1.5108, -0.7592, -1.0533,\n",
       "          -0.5271, -0.0379,  1.5924],\n",
       "         [ 0.1004, -0.4802, -0.9359,  0.6083,  0.7245, -0.2818,  1.2587,\n",
       "          -0.9187, -1.0562,  0.5173]],\n",
       "\n",
       "        [[ 0.1083, -0.4574,  0.0858,  3.3604,  0.5518, -1.0142,  0.3925,\n",
       "           0.6480,  1.1868, -1.4028],\n",
       "         [ 1.6098,  0.5863, -0.0314,  0.2103,  0.7839,  2.4461,  0.3393,\n",
       "          -0.5896, -0.5009,  0.4924],\n",
       "         [ 0.9974, -3.2560,  0.9630,  0.9420,  0.9524, -0.8453, -0.8829,\n",
       "          -0.1975,  0.8842, -1.2241]],\n",
       "\n",
       "        [[ 1.6292,  0.3791,  0.4096,  1.1459,  0.8005, -0.2486, -0.6314,\n",
       "           1.1429, -0.3846,  0.5391],\n",
       "         [ 0.8014, -0.2091,  1.6413, -1.4698,  1.8569, -1.4765, -0.2131,\n",
       "           1.8491,  0.9819, -0.2590],\n",
       "         [ 0.6215,  1.4780,  1.3618, -0.2897, -0.4878,  0.7336,  1.5624,\n",
       "          -0.9963,  0.3498, -1.1485]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "input = torch.randn(5, 3,10)\n",
    "input\n",
    "# batch, row, coloumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(10,20, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(10, 20, num_layers=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.randn(2,3,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5996,  0.2440, -0.5369,  1.4706, -0.0229,  1.6173,  1.5830,\n",
       "          -0.1642, -0.0112, -0.3083,  0.4081,  0.3066,  1.3025, -0.8954,\n",
       "          -1.3332,  0.6373,  0.1867, -0.6263,  0.9040,  0.6838],\n",
       "         [-1.3037, -0.7867,  0.1945,  0.2044,  1.7548, -1.0303, -1.3641,\n",
       "           0.6604,  0.2993,  1.7027, -0.3290,  0.0372, -1.1426,  0.1028,\n",
       "          -0.5673,  0.2967,  1.3292, -1.8374, -0.1199,  0.9783],\n",
       "         [-0.9179,  0.6312,  0.1442, -0.1914, -0.1751, -0.1297, -0.3323,\n",
       "          -0.3465,  0.5814, -0.2075,  0.4553, -0.2455, -0.2824,  0.0291,\n",
       "          -2.5605, -0.1180,  0.7881,  0.7495,  1.2218, -0.8857]],\n",
       "\n",
       "        [[ 2.6250, -1.0780,  0.1782, -1.7311,  0.4933, -0.8868, -0.0901,\n",
       "           0.0348, -0.0034,  1.5636, -0.8918, -0.1658,  1.1308,  0.4559,\n",
       "           1.1487, -0.2816,  0.6065, -1.7440,  1.1548, -0.5164],\n",
       "         [-0.7567,  0.3737, -0.5100, -0.8625, -0.0104,  0.8632,  1.7742,\n",
       "          -0.0511, -1.0769, -0.2410, -0.2307, -0.2506, -0.3763, -0.1372,\n",
       "           1.3877, -1.2697,  0.2571, -0.6031,  0.4764, -1.3173],\n",
       "         [ 0.9916, -0.3676,  1.1752,  1.3670, -0.3067, -1.4833, -1.2632,\n",
       "           0.6887, -0.3806,  1.8336,  1.9736,  1.4811,  0.4749, -1.2406,\n",
       "          -1.5346,  0.0080,  1.3240,  0.0441,  1.6700, -1.0696]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 20])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.1692e-01,  3.3775e-01,  1.0347e+00, -5.0556e-01,  6.3558e-01,\n",
      "         -2.2188e-01, -2.9829e-01,  2.1201e-01, -1.5060e-01, -1.4928e+00,\n",
      "          1.0425e+00, -6.3973e-01, -3.1927e+00,  4.4534e-01, -9.0239e-01,\n",
      "          9.9398e-02],\n",
      "        [-1.4698e+00,  4.5913e-01, -1.1272e+00,  2.7887e-01,  5.8610e-01,\n",
      "         -1.1392e+00, -8.3770e-01, -7.2372e-01,  6.0096e-01,  4.3577e-01,\n",
      "          6.3487e-01, -1.4629e+00,  5.6206e-01, -2.1030e-01, -9.3288e-01,\n",
      "         -8.2281e-01],\n",
      "        [-9.7664e-01, -7.6996e-01, -8.7964e-01,  3.4571e-01, -3.5144e-01,\n",
      "         -2.9372e-02, -5.8434e-01, -1.4216e+00, -3.0299e-01, -1.5231e+00,\n",
      "         -2.1749e-02,  1.6886e-01, -7.8691e-01, -5.1211e-01,  4.5315e-01,\n",
      "         -6.5712e-01],\n",
      "        [-1.7187e-01,  1.1224e-01,  2.2634e+00,  4.5089e-01,  6.6332e-01,\n",
      "         -5.6883e-01, -2.0975e+00, -2.8912e-01,  1.8391e+00, -2.1161e-01,\n",
      "         -1.3850e-01, -2.9966e-01, -3.2739e-01, -8.4278e-03, -6.9303e-01,\n",
      "          2.3019e+00],\n",
      "        [-1.2751e+00, -2.0265e+00,  1.4160e-01, -1.9704e+00,  3.7565e-01,\n",
      "         -1.5678e+00,  3.8167e-01, -6.1732e-01,  4.3286e-01,  1.2210e+00,\n",
      "          1.7976e+00,  7.1585e-02,  5.8750e-02, -1.1044e+00, -5.6278e-01,\n",
      "         -2.1852e-02],\n",
      "        [ 2.0920e-01, -6.9153e-01,  1.6072e+00, -3.5015e-02,  2.6894e-01,\n",
      "         -1.5742e+00,  2.4040e+00,  1.0617e+00, -4.5715e-01,  9.0566e-02,\n",
      "         -1.2372e+00,  1.6468e-01,  1.3079e+00,  1.4589e+00, -1.0038e+00,\n",
      "         -3.7963e-01],\n",
      "        [ 4.5299e-02,  1.9667e-01,  5.2204e-01, -1.6529e-01, -1.7841e-01,\n",
      "         -4.7263e-02,  1.2841e+00, -9.4852e-01, -7.7525e-01,  6.5910e-01,\n",
      "          3.7959e-02, -5.2457e-01, -1.9620e-01,  2.4991e-01,  2.0098e-01,\n",
      "          7.9803e-01],\n",
      "        [-1.0644e-01,  1.3898e+00,  1.1140e+00, -1.1806e+00, -2.8635e-01,\n",
      "          1.5241e-01, -3.7300e-02, -2.3806e+00,  4.6347e-01,  9.4488e-01,\n",
      "          1.0859e+00,  2.0432e+00,  1.4069e-02, -7.7073e-02, -2.0147e+00,\n",
      "          5.3929e-01],\n",
      "        [ 1.3042e-01, -8.4036e-01, -1.6669e+00, -3.4914e-01,  2.2327e+00,\n",
      "         -4.7006e-01,  7.4223e-01, -1.7652e+00, -9.3021e-01,  5.8107e-01,\n",
      "         -6.2025e-02, -5.8701e-01, -1.3527e+00, -5.5904e-01,  2.8101e-01,\n",
      "          1.8938e-03],\n",
      "        [ 1.3801e+00, -6.5355e-01, -2.3647e-01, -7.2003e-01, -1.8044e+00,\n",
      "          5.3301e-01,  1.0332e+00,  5.9653e-01, -2.6452e-01,  1.2059e+00,\n",
      "         -1.4042e+00, -1.2551e+00, -2.5930e+00,  8.9139e-01,  1.2146e+00,\n",
      "          4.6606e-01],\n",
      "        [-1.5426e+00, -2.6172e-01,  1.0890e+00,  5.3023e-01,  1.5724e-01,\n",
      "         -5.4575e-01, -4.3990e-01, -1.4265e-01, -1.2889e+00,  7.7221e-01,\n",
      "          5.5539e-01, -1.8776e+00, -4.0892e-01, -6.2084e-01,  3.8611e-01,\n",
      "         -1.6864e+00],\n",
      "        [-3.3393e-01,  1.3818e+00, -3.8287e-01,  1.1879e+00, -4.6637e-01,\n",
      "         -2.9898e-01, -1.6032e+00,  2.3538e-01, -7.0436e-01, -8.8248e-01,\n",
      "         -1.4948e+00, -6.2800e-01, -1.4004e+00, -2.3828e-01, -6.3405e-01,\n",
      "         -2.7911e-01],\n",
      "        [ 1.1267e+00, -6.6632e-01, -1.8311e+00, -5.6738e-01,  7.4985e-01,\n",
      "         -1.5104e+00,  8.2284e-01, -6.9022e-01,  1.4215e+00,  1.1914e-01,\n",
      "         -6.0949e-01,  1.5019e-01, -3.8057e-01, -1.6420e+00,  2.6884e-01,\n",
      "         -1.8212e-01],\n",
      "        [-2.4535e+00, -2.1042e-01,  1.0580e-01,  1.5633e-01, -9.0217e-01,\n",
      "          6.0600e-02,  1.1382e+00,  1.5712e+00,  3.7750e-01, -2.3273e-01,\n",
      "         -1.9966e+00,  1.1514e-01, -4.2771e-01,  1.3649e+00, -2.2058e+00,\n",
      "          1.4125e+00],\n",
      "        [-2.7489e+00, -1.5313e-01,  3.4538e-01, -1.0334e+00, -1.5625e+00,\n",
      "         -6.0609e-02, -9.7307e-01, -5.4485e-01,  2.2312e-01,  7.9502e-01,\n",
      "          2.2592e+00,  3.6797e-01, -9.2226e-01, -4.0748e-01, -6.5527e-02,\n",
      "          1.0035e+00],\n",
      "        [-8.5315e-01,  7.1142e-01,  1.8475e-01,  1.0442e+00,  1.0795e+00,\n",
      "          1.0141e+00, -3.2095e-01,  4.5491e-01,  1.5581e+00,  1.6719e+00,\n",
      "          4.2937e-01,  5.4629e-01, -5.8112e-02, -1.6093e+00,  9.4254e-01,\n",
      "          1.1084e+00],\n",
      "        [-2.0969e-01, -2.0413e+00, -1.0754e+00,  9.3104e-01,  1.9963e+00,\n",
      "          1.9488e-01,  1.7259e-02, -2.4079e-01, -6.6130e-01,  3.4065e+00,\n",
      "         -4.0139e-01, -5.6995e-01, -5.6405e-01, -1.3252e+00, -3.5095e-01,\n",
      "          8.8082e-01],\n",
      "        [-9.6265e-01,  9.0538e-01,  1.9311e+00,  5.8426e-01,  1.2249e+00,\n",
      "         -1.4001e+00,  1.6434e+00,  7.9450e-02,  8.2716e-02, -1.1864e+00,\n",
      "         -6.0199e-01,  1.6082e-01, -1.4868e-02,  2.7282e-01, -8.6991e-01,\n",
      "         -4.2126e-01],\n",
      "        [-9.3867e-01,  1.1533e+00,  1.2197e-01,  5.7933e-01,  4.9950e-02,\n",
      "          2.9504e-02, -1.2591e+00, -1.4126e-01,  6.3934e-01,  2.5718e-02,\n",
      "         -1.3550e+00, -9.5657e-02, -9.6900e-02,  1.1532e+00, -3.2245e-01,\n",
      "          6.1721e-01],\n",
      "        [-5.8497e-01,  1.3549e+00,  1.9536e+00, -1.9190e+00,  2.4009e-01,\n",
      "         -2.0237e-01,  3.4271e-01,  1.8986e-01,  1.0979e+00, -7.3605e-01,\n",
      "          2.9118e-02,  1.4789e+00, -1.6759e+00, -1.7065e-01,  2.0324e-01,\n",
      "          6.1927e-01]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-5.2115e-01,  4.2219e-01,  0.0000e+00, -0.0000e+00,  7.9448e-01,\n",
       "         -2.7735e-01, -3.7287e-01,  0.0000e+00, -1.8824e-01, -1.8660e+00,\n",
       "          0.0000e+00, -7.9966e-01, -3.9909e+00,  5.5667e-01, -1.1280e+00,\n",
       "          1.2425e-01],\n",
       "        [-1.8373e+00,  5.7391e-01, -1.4090e+00,  3.4859e-01,  7.3263e-01,\n",
       "         -1.4239e+00, -0.0000e+00, -9.0464e-01,  7.5120e-01,  0.0000e+00,\n",
       "          0.0000e+00, -1.8286e+00,  7.0258e-01, -2.6288e-01, -1.1661e+00,\n",
       "         -1.0285e+00],\n",
       "        [-1.2208e+00, -9.6245e-01, -1.0996e+00,  4.3214e-01, -4.3929e-01,\n",
       "         -3.6715e-02, -0.0000e+00, -1.7770e+00, -3.7874e-01, -1.9039e+00,\n",
       "         -0.0000e+00,  2.1108e-01, -9.8364e-01, -0.0000e+00,  5.6643e-01,\n",
       "         -8.2140e-01],\n",
       "        [-0.0000e+00,  1.4030e-01,  2.8292e+00,  5.6361e-01,  8.2915e-01,\n",
       "         -7.1104e-01, -2.6219e+00, -0.0000e+00,  2.2988e+00, -0.0000e+00,\n",
       "         -0.0000e+00, -3.7457e-01, -4.0924e-01, -1.0535e-02, -8.6629e-01,\n",
       "          2.8774e+00],\n",
       "        [-0.0000e+00, -2.5331e+00,  1.7700e-01, -2.4630e+00,  4.6957e-01,\n",
       "         -1.9597e+00,  0.0000e+00, -7.7164e-01,  5.4107e-01,  0.0000e+00,\n",
       "          0.0000e+00,  8.9482e-02,  7.3438e-02, -1.3804e+00, -7.0348e-01,\n",
       "         -2.7315e-02],\n",
       "        [ 2.6150e-01, -8.6441e-01,  0.0000e+00, -4.3769e-02,  3.3617e-01,\n",
       "         -1.9678e+00,  3.0050e+00,  0.0000e+00, -5.7143e-01,  1.1321e-01,\n",
       "         -0.0000e+00,  2.0585e-01,  1.6349e+00,  0.0000e+00, -1.2547e+00,\n",
       "         -0.0000e+00],\n",
       "        [ 5.6624e-02,  2.4583e-01,  0.0000e+00, -2.0661e-01, -2.2301e-01,\n",
       "         -5.9079e-02,  1.6051e+00, -1.1857e+00, -9.6906e-01,  8.2388e-01,\n",
       "          4.7448e-02, -6.5572e-01, -0.0000e+00,  3.1238e-01,  2.5123e-01,\n",
       "          9.9753e-01],\n",
       "        [-0.0000e+00,  1.7373e+00,  0.0000e+00, -1.4757e+00, -0.0000e+00,\n",
       "          1.9051e-01, -4.6625e-02, -2.9757e+00,  0.0000e+00,  0.0000e+00,\n",
       "          1.3574e+00,  2.5541e+00,  1.7586e-02, -9.6341e-02, -2.5184e+00,\n",
       "          6.7411e-01],\n",
       "        [ 1.6303e-01, -1.0504e+00, -2.0837e+00, -4.3643e-01,  2.7908e+00,\n",
       "         -5.8758e-01,  9.2779e-01, -2.2066e+00, -1.1628e+00,  7.2633e-01,\n",
       "         -7.7531e-02, -7.3376e-01, -1.6909e+00, -6.9880e-01,  3.5126e-01,\n",
       "          2.3672e-03],\n",
       "        [ 1.7251e+00, -0.0000e+00, -2.9558e-01, -9.0003e-01, -2.2555e+00,\n",
       "          6.6626e-01,  1.2915e+00,  7.4566e-01, -3.3065e-01,  1.5074e+00,\n",
       "         -1.7553e+00, -0.0000e+00, -3.2413e+00,  1.1142e+00,  1.5183e+00,\n",
       "          5.8258e-01],\n",
       "        [-0.0000e+00, -3.2715e-01,  1.3612e+00,  0.0000e+00,  1.9655e-01,\n",
       "         -6.8219e-01, -5.4988e-01, -1.7832e-01, -1.6111e+00,  0.0000e+00,\n",
       "          0.0000e+00, -2.3470e+00, -5.1115e-01, -7.7605e-01,  0.0000e+00,\n",
       "         -2.1081e+00],\n",
       "        [-0.0000e+00,  1.7273e+00, -4.7859e-01,  1.4849e+00, -5.8296e-01,\n",
       "         -3.7373e-01, -0.0000e+00,  2.9423e-01, -8.8046e-01, -1.1031e+00,\n",
       "         -1.8685e+00, -7.8501e-01, -1.7505e+00, -2.9785e-01, -7.9257e-01,\n",
       "         -0.0000e+00],\n",
       "        [ 1.4084e+00, -8.3290e-01, -2.2889e+00, -0.0000e+00,  9.3731e-01,\n",
       "         -1.8880e+00,  1.0286e+00, -0.0000e+00,  1.7769e+00,  0.0000e+00,\n",
       "         -0.0000e+00,  1.8774e-01, -0.0000e+00, -0.0000e+00,  3.3605e-01,\n",
       "         -2.2765e-01],\n",
       "        [-0.0000e+00, -2.6303e-01,  1.3225e-01,  1.9541e-01, -1.1277e+00,\n",
       "          0.0000e+00,  1.4227e+00,  1.9640e+00,  4.7188e-01, -2.9091e-01,\n",
       "         -2.4957e+00,  1.4392e-01, -5.3464e-01,  1.7062e+00, -2.7573e+00,\n",
       "          1.7656e+00],\n",
       "        [-3.4361e+00, -1.9141e-01,  0.0000e+00, -1.2918e+00, -1.9531e+00,\n",
       "         -7.5761e-02, -1.2163e+00, -6.8107e-01,  2.7890e-01,  9.9378e-01,\n",
       "          2.8240e+00,  4.5996e-01, -1.1528e+00, -5.0935e-01, -8.1908e-02,\n",
       "          1.2543e+00],\n",
       "        [-1.0664e+00,  8.8928e-01,  2.3094e-01,  0.0000e+00,  1.3493e+00,\n",
       "          1.2676e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          5.3671e-01,  6.8287e-01, -0.0000e+00, -2.0117e+00,  1.1782e+00,\n",
       "          1.3854e+00],\n",
       "        [-2.6212e-01, -2.5516e+00, -1.3442e+00,  1.1638e+00,  0.0000e+00,\n",
       "          2.4360e-01,  2.1574e-02, -0.0000e+00, -8.2663e-01,  0.0000e+00,\n",
       "         -5.0174e-01, -0.0000e+00, -0.0000e+00, -1.6565e+00, -4.3869e-01,\n",
       "          1.1010e+00],\n",
       "        [-0.0000e+00,  1.1317e+00,  2.4139e+00,  7.3033e-01,  1.5311e+00,\n",
       "         -1.7501e+00,  2.0542e+00,  9.9312e-02,  1.0340e-01, -1.4830e+00,\n",
       "         -0.0000e+00,  2.0102e-01, -1.8585e-02,  3.4102e-01, -0.0000e+00,\n",
       "         -0.0000e+00],\n",
       "        [-1.1733e+00,  1.4416e+00,  1.5246e-01,  7.2416e-01,  0.0000e+00,\n",
       "          3.6881e-02, -0.0000e+00, -1.7657e-01,  7.9917e-01,  0.0000e+00,\n",
       "         -1.6937e+00, -1.1957e-01, -1.2113e-01,  0.0000e+00, -0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [-7.3121e-01,  1.6937e+00,  2.4420e+00, -2.3987e+00,  3.0011e-01,\n",
       "         -2.5296e-01,  4.2839e-01,  2.3733e-01,  1.3723e+00, -9.2006e-01,\n",
       "          0.0000e+00,  1.8486e+00, -2.0949e+00, -2.1331e-01,  2.5405e-01,\n",
       "          7.7409e-01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Dropout(p=0.2)\n",
    "input = torch.randn(20, 16)\n",
    "print(input)\n",
    "output = m(input)\n",
    "output\n",
    "# The nn.Linear layer is a fully connected (dense) layer that maps the output of the LSTM to a probability distribution over the vocabulary. \n",
    "# This is a standard practice when predicting categorical values (in this case, words in a vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "            Forward pass of the encoder\n",
    "            Arguments:\n",
    "            - Features: Tensor of shape (batch_size, feature_size=512)\n",
    "            - caption: Tensor of shape (batch_size, max_caption_length), word indices\n",
    "            Returns:\n",
    "            - output: Tensor of shape (batch_size, max_caption_length, vocab_size), word prediction\n",
    "        \"\"\"\n",
    "        # Embedding the caption, excluding the <end> token\n",
    "        embedding = self.embedding(captions[:, :-1])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # Embedding layer: converts word indices into dense vectors of size embed_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # LSTM: input to hidden, hidden_size must match the size of features from CNN\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to map LSTM output to vocab_size\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Initialize the hidden state (if needed)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Optional dropout to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "        Arguments:\n",
    "        - features: Tensor of shape (batch_size, feature_size=512)\n",
    "        - captions: Tensor of shape (batch_size, max_caption_length), word indices\n",
    "        \n",
    "        Returns:\n",
    "        - outputs: Tensor of shape (batch_size, max_caption_length, vocab_size), word predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        # Embedding the captions, excluding the <end> token\"\n",
    "        embeddings = self.embedding(captions[:, :-1])\n",
    "        \n",
    "        # Concatenate the features with the embedded captions\n",
    "        # Features are passed as input to the first time step\n",
    "        features = features.unsqueeze(1)  # shape (batch_size, 1, feature_size)\n",
    "        lstm_input = torch.cat((features, embeddings), 1)  # shape (batch_size, 1 + caption_length, embed_size)\n",
    "        \n",
    "        # Pass the concatenated inputs through the LSTM\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        \n",
    "        # Pass the LSTM output through the fully connected layer to get word predictions\n",
    "        outputs = self.fc(lstm_out)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m embed_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m      5\u001b[0m hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[0;32m----> 6\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mvocab\u001b[49m)  \u001b[38;5;66;03m# Vocabulary size\u001b[39;00m\n\u001b[1;32m      7\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      8\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab)  # Vocabulary size\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "log_interval = 10  # Log every 10 batches\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers=1)\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss is good for multi-class classification\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Assume you have a pretrained CNN (e.g., ResNet) model as your feature extractor\n",
    "cnn_model = pretrained_resnet_model  # The CNN for image feature extraction\n",
    "\n",
    "# Set the CNN to evaluation mode (to avoid updating its weights)\n",
    "cnn_model.eval()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    decoder.train()  # Set the decoder to training mode\n",
    "    \n",
    "    for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "        \n",
    "        # Forward pass: Pass the images through the CNN to get features\n",
    "        with torch.no_grad():  # No need to compute gradients for CNN\n",
    "            features = cnn_model(images)  # (batch_size, feature_size=512)\n",
    "\n",
    "        # Zero the gradients for the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: Pass the features and captions through the RNN\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Compute the loss between the RNN outputs and the target captions\n",
    "        # We need to reshape the output to (batch_size * max_caption_length, vocab_size)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].reshape(-1))  # Exclude <start> token in targets\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model's parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging the loss every log_interval\n",
    "        if i % log_interval == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(data_loader)}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagecapenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
