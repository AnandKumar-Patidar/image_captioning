{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "class Vocabulary:\n",
    "    def __init__(self, threshold):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # Special Tokesn\n",
    "        self.add_word('<pad>')\n",
    "        self.add_word('<start>')\n",
    "        self.add_word('<end>')\n",
    "        self.add_word('<unk>')\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx+=1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the vocabulary.\"\"\"\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(ann_file, threshold=5):\n",
    "    from pycocotools.coco import COCO\n",
    "    coco = COCO(ann_file)\n",
    "    counter = defaultdict(int)\n",
    "    for ann_id in coco.anns.keys():\n",
    "        caption = coco.anns[ann_id]['caption']\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        for token in tokens:\n",
    "            counter[token]+=1\n",
    "\n",
    "    # create vocab\n",
    "    vocab = Vocabulary(threshold)\n",
    "    for word, count in counter.items():\n",
    "        if count>threshold:\n",
    "            vocab.add_word(word)\n",
    "\n",
    "    return vocab\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab('/home/mirsee/projects/image_captioning/notebooks/annotations/captions_val2017.json', threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<start>': 1,\n",
       " '<end>': 2,\n",
       " '<unk>': 3,\n",
       " 'a': 4,\n",
       " 'black': 5,\n",
       " 'honda': 6,\n",
       " 'motorcycle': 7,\n",
       " 'parked': 8,\n",
       " 'in': 9,\n",
       " 'front': 10,\n",
       " 'of': 11,\n",
       " 'garage': 12,\n",
       " '.': 13,\n",
       " 'grass': 14,\n",
       " 'driveway': 15,\n",
       " 'an': 16,\n",
       " 'office': 17,\n",
       " 'cubicle': 18,\n",
       " 'with': 19,\n",
       " 'four': 20,\n",
       " 'different': 21,\n",
       " 'types': 22,\n",
       " 'computers': 23,\n",
       " 'small': 24,\n",
       " 'closed': 25,\n",
       " 'toilet': 26,\n",
       " 'cramped': 27,\n",
       " 'space': 28,\n",
       " 'two': 29,\n",
       " 'women': 30,\n",
       " 'waiting': 31,\n",
       " 'at': 32,\n",
       " 'bench': 33,\n",
       " 'next': 34,\n",
       " 'to': 35,\n",
       " 'street': 36,\n",
       " 'dark': 37,\n",
       " 'burgundy': 38,\n",
       " 'seat': 39,\n",
       " 'tan': 40,\n",
       " 'and': 41,\n",
       " 'sink': 42,\n",
       " 'combination': 43,\n",
       " 'room': 44,\n",
       " 'the': 45,\n",
       " 'home': 46,\n",
       " 'seems': 47,\n",
       " 'be': 48,\n",
       " 'very': 49,\n",
       " 'cluttered': 50,\n",
       " 'beautiful': 51,\n",
       " 'dessert': 52,\n",
       " 'by': 53,\n",
       " 'people': 54,\n",
       " 'woman': 55,\n",
       " 'sitting': 56,\n",
       " 'on': 57,\n",
       " 'standing': 58,\n",
       " 'for': 59,\n",
       " 'bus': 60,\n",
       " 'middle': 61,\n",
       " 'city': 62,\n",
       " 'this': 63,\n",
       " 'is': 64,\n",
       " 'control': 65,\n",
       " 'panel': 66,\n",
       " 'cat': 67,\n",
       " 'eating': 68,\n",
       " 'bird': 69,\n",
       " 'it': 70,\n",
       " 'has': 71,\n",
       " 'caught': 72,\n",
       " 'close-up': 73,\n",
       " 'picture': 74,\n",
       " 'fountain': 75,\n",
       " 'shot': 76,\n",
       " 'elderly': 77,\n",
       " 'man': 78,\n",
       " 'inside': 79,\n",
       " 'kitchen': 80,\n",
       " 'between': 81,\n",
       " 'cars': 82,\n",
       " 'parking': 83,\n",
       " 'lot': 84,\n",
       " 'behind': 85,\n",
       " 'stop': 86,\n",
       " 'multiple': 87,\n",
       " 'old': 88,\n",
       " 'wearing': 89,\n",
       " 'odd': 90,\n",
       " 'hat': 91,\n",
       " 'parade': 92,\n",
       " 'motorcycles': 93,\n",
       " 'going': 94,\n",
       " 'through': 95,\n",
       " 'group': 96,\n",
       " 'tall': 97,\n",
       " 'trees': 98,\n",
       " 'sleeping': 99,\n",
       " 'his': 100,\n",
       " 'him': 101,\n",
       " 'all': 102,\n",
       " 'white': 103,\n",
       " 'electric': 104,\n",
       " 'desk': 105,\n",
       " 'computer': 106,\n",
       " 'chair': 107,\n",
       " 'laptop': 108,\n",
       " 'telephone': 109,\n",
       " 'banana': 110,\n",
       " 'where': 111,\n",
       " 'receiver': 112,\n",
       " 'off': 113,\n",
       " 'faucet': 114,\n",
       " 'controls': 115,\n",
       " 'stove': 116,\n",
       " 'sits': 117,\n",
       " 'counter': 118,\n",
       " 'tops': 119,\n",
       " 'another': 120,\n",
       " 'setting': 121,\n",
       " 'screens': 122,\n",
       " 'close': 123,\n",
       " 'up': 124,\n",
       " 'setup': 125,\n",
       " 'coffee': 126,\n",
       " 'maker': 127,\n",
       " 'cabinet': 128,\n",
       " 'there': 129,\n",
       " 'piece': 130,\n",
       " 'cake': 131,\n",
       " 'plate': 132,\n",
       " 'decorations': 133,\n",
       " 'stands': 134,\n",
       " 'grassy': 135,\n",
       " 'sidewalk': 136,\n",
       " 'cheesecake': 137,\n",
       " 'whip': 138,\n",
       " 'cream': 139,\n",
       " 'motorcyclists': 140,\n",
       " 'drive': 141,\n",
       " 'down': 142,\n",
       " 'tree': 143,\n",
       " 'lined': 144,\n",
       " 'dog': 145,\n",
       " 'its': 146,\n",
       " 'feet': 147,\n",
       " 'watching': 148,\n",
       " 'tv': 149,\n",
       " 'young': 150,\n",
       " 'cute': 151,\n",
       " 'enjoy': 152,\n",
       " 'nap': 153,\n",
       " 'together': 154,\n",
       " 'person': 155,\n",
       " 'looking': 156,\n",
       " 'older': 157,\n",
       " 'funny': 158,\n",
       " 'dining': 159,\n",
       " 'bikes': 160,\n",
       " 'bikers': 161,\n",
       " ',': 162,\n",
       " 'dressed': 163,\n",
       " 'their': 164,\n",
       " 'gear': 165,\n",
       " 'near': 166,\n",
       " 'head': 167,\n",
       " 'pillow': 168,\n",
       " 'phone': 169,\n",
       " 'machine': 170,\n",
       " 'gravel': 171,\n",
       " 'are': 172,\n",
       " 'television': 173,\n",
       " 'extended': 174,\n",
       " 'outside': 175,\n",
       " 'long': 176,\n",
       " 'filled': 177,\n",
       " 'either': 178,\n",
       " 'side': 179,\n",
       " 'flat': 180,\n",
       " 'monitors': 181,\n",
       " 'grey': 182,\n",
       " 'watches': 183,\n",
       " 'from': 184,\n",
       " 'men': 185,\n",
       " 'stand': 186,\n",
       " 'bicycles': 187,\n",
       " 'three': 188,\n",
       " 'around': 189,\n",
       " 'bedroom': 190,\n",
       " 'set': 191,\n",
       " 'toy': 192,\n",
       " 'animals': 193,\n",
       " 'red': 194,\n",
       " 'wooden': 195,\n",
       " 'wagon': 196,\n",
       " '&': 197,\n",
       " 'pink': 198,\n",
       " 'top': 199,\n",
       " 'takes': 200,\n",
       " 'selfie': 201,\n",
       " 'bathroom': 202,\n",
       " 'mirror': 203,\n",
       " 'replaced': 204,\n",
       " 'patio': 205,\n",
       " 'bed': 206,\n",
       " 'cabinets': 207,\n",
       " 'appliances': 208,\n",
       " 'striped': 209,\n",
       " 'plane': 210,\n",
       " 'flying': 211,\n",
       " 'into': 212,\n",
       " 'sky': 213,\n",
       " 'as': 214,\n",
       " 'sun': 215,\n",
       " 'shines': 216,\n",
       " 'driving': 217,\n",
       " 'suv': 218,\n",
       " 'open': 219,\n",
       " 'covered': 220,\n",
       " 'field': 221,\n",
       " 'customized': 222,\n",
       " 'more': 223,\n",
       " 'background': 224,\n",
       " 'airplane': 225,\n",
       " 'ascending': 226,\n",
       " 'sculpture': 227,\n",
       " 'stting': 228,\n",
       " 'purses': 229,\n",
       " 'ground': 230,\n",
       " 'while': 231,\n",
       " 'line': 232,\n",
       " 'them': 233,\n",
       " 'american': 234,\n",
       " 'airlines': 235,\n",
       " 'house': 236,\n",
       " 'door': 237,\n",
       " 'step': 238,\n",
       " 'overlooking': 239,\n",
       " 'water': 240,\n",
       " 'metal': 241,\n",
       " 'statue': 242,\n",
       " 'little': 243,\n",
       " 'traffic': 244,\n",
       " 'intersection': 245,\n",
       " 'books': 246,\n",
       " 'photograph': 247,\n",
       " 'placed': 248,\n",
       " 'beside': 249,\n",
       " 'each': 250,\n",
       " 'other': 251,\n",
       " 'orange': 252,\n",
       " 'rose': 253,\n",
       " 'vase': 254,\n",
       " 'table': 255,\n",
       " \"'s\": 256,\n",
       " 'taken': 257,\n",
       " 'blue': 258,\n",
       " 'tabby': 259,\n",
       " 'crouched': 260,\n",
       " 'flowered': 261,\n",
       " 'dress': 262,\n",
       " 'taking': 263,\n",
       " 'photo': 264,\n",
       " 'trash': 265,\n",
       " 'bag': 266,\n",
       " 'restroom': 267,\n",
       " 'jacket': 268,\n",
       " 'looks': 269,\n",
       " 'camera': 270,\n",
       " 'several': 271,\n",
       " 'bull': 272,\n",
       " 'giraffe': 273,\n",
       " 'deer': 274,\n",
       " 'flower': 275,\n",
       " 'self': 276,\n",
       " 'jeep': 277,\n",
       " 'clean': 278,\n",
       " 'windows': 279,\n",
       " 'some': 280,\n",
       " 'urban': 281,\n",
       " 'stoplights': 282,\n",
       " 'cloudy': 283,\n",
       " 'day': 284,\n",
       " 'relaxing': 285,\n",
       " 'children': 286,\n",
       " 'strewn': 287,\n",
       " 'across': 288,\n",
       " 'floor': 289,\n",
       " 'glass': 290,\n",
       " 'bowl': 291,\n",
       " 'oranges': 292,\n",
       " 'attractive': 293,\n",
       " 'toilette': 294,\n",
       " 'using': 295,\n",
       " 'bunk': 296,\n",
       " 'made': 297,\n",
       " 'display': 298,\n",
       " 'riders': 299,\n",
       " 'stomach': 300,\n",
       " 'pose': 301,\n",
       " 'victorian': 302,\n",
       " 'style': 303,\n",
       " 'frame': 304,\n",
       " 'flies': 305,\n",
       " 'angle': 306,\n",
       " 'large': 307,\n",
       " 'rear': 308,\n",
       " 'skinny': 309,\n",
       " 'tire': 310,\n",
       " 'view': 311,\n",
       " 'that': 312,\n",
       " 'garbage': 313,\n",
       " 'lighted': 314,\n",
       " 'posing': 315,\n",
       " 'haired': 316,\n",
       " 'dead': 317,\n",
       " 'apples': 318,\n",
       " 'stool': 319,\n",
       " 'depicting': 320,\n",
       " 'security': 321,\n",
       " 'cameras': 322,\n",
       " 'vintage': 323,\n",
       " 'animal': 324,\n",
       " 'toys': 325,\n",
       " 'facing': 326,\n",
       " 'out': 327,\n",
       " 'corner': 328,\n",
       " 'bike': 329,\n",
       " 'bicycle': 330,\n",
       " 'riding': 331,\n",
       " 'mopeds': 332,\n",
       " 'park': 333,\n",
       " 'stripped': 334,\n",
       " 'brick': 335,\n",
       " 'wall': 336,\n",
       " 'light': 337,\n",
       " 'yellow': 338,\n",
       " 'clear': 339,\n",
       " 'toddler': 340,\n",
       " 'celebrates': 341,\n",
       " 'birthday': 342,\n",
       " 'cupcake': 343,\n",
       " 'girl': 344,\n",
       " 'bottom': 345,\n",
       " 'her': 346,\n",
       " 'vanity': 347,\n",
       " 'shower': 348,\n",
       " 'holding': 349,\n",
       " 'flowers': 350,\n",
       " 'zebra': 351,\n",
       " 'checking': 352,\n",
       " 'spacious': 353,\n",
       " 'brown': 354,\n",
       " 'cabinetry': 355,\n",
       " 'car': 356,\n",
       " 'bucket': 357,\n",
       " 'truck': 358,\n",
       " 'drivers': 359,\n",
       " 'medicine': 360,\n",
       " 'toothbrush': 361,\n",
       " 'holder': 362,\n",
       " 'bare': 363,\n",
       " 'walls': 364,\n",
       " 'chained': 365,\n",
       " 'pole': 366,\n",
       " 'train': 367,\n",
       " 'station': 368,\n",
       " 'back': 369,\n",
       " 'desktop': 370,\n",
       " 'beds': 371,\n",
       " 'full': 372,\n",
       " 'boy': 373,\n",
       " 'spoon': 374,\n",
       " 'walking': 375,\n",
       " 'african': 376,\n",
       " 'plain': 377,\n",
       " 'fruit': 378,\n",
       " 'seen': 379,\n",
       " 'herself': 380,\n",
       " 'scooters': 381,\n",
       " 'rides': 382,\n",
       " 'one': 383,\n",
       " 'who': 384,\n",
       " 'share': 385,\n",
       " 'apple': 386,\n",
       " 'sit': 387,\n",
       " 'platform': 388,\n",
       " 'no': 389,\n",
       " 'food': 390,\n",
       " 'lap': 391,\n",
       " 'monitor': 392,\n",
       " 'locked': 393,\n",
       " 'waits': 394,\n",
       " 'board': 395,\n",
       " 'mass': 396,\n",
       " 'transit': 397,\n",
       " 'baby': 398,\n",
       " 'candle': 399,\n",
       " 'wild': 400,\n",
       " 'operating': 401,\n",
       " 'wash': 402,\n",
       " 'runway': 403,\n",
       " 'just': 404,\n",
       " 'landed': 405,\n",
       " 'or': 406,\n",
       " 'ready': 407,\n",
       " 'take': 408,\n",
       " 'extremely': 409,\n",
       " 'colored': 410,\n",
       " 'child': 411,\n",
       " 'holds': 412,\n",
       " 'scene': 413,\n",
       " 'shown': 414,\n",
       " 'dry': 415,\n",
       " 'kites': 416,\n",
       " 'overhead': 417,\n",
       " 'washing': 418,\n",
       " 'motor': 419,\n",
       " 'cycles': 420,\n",
       " 'cross': 421,\n",
       " 'downtown': 422,\n",
       " 'area': 423,\n",
       " 'landing': 424,\n",
       " 'airport': 425,\n",
       " 'being': 426,\n",
       " 'flown': 427,\n",
       " 'track': 428,\n",
       " 'deserted': 429,\n",
       " 'tower': 430,\n",
       " 'pickup': 431,\n",
       " 'underneath': 432,\n",
       " 'smoke': 433,\n",
       " '..': 434,\n",
       " 'part': 435,\n",
       " 'connected': 436,\n",
       " 'covering': 437,\n",
       " 'doorway': 438,\n",
       " 'colorful': 439,\n",
       " 'stopped': 440,\n",
       " 'sliding': 441,\n",
       " 'single': 442,\n",
       " 'passanger': 443,\n",
       " 'clothes': 444,\n",
       " 'washer': 445,\n",
       " 'passenger': 446,\n",
       " 'air': 447,\n",
       " 'canopy': 448,\n",
       " 'see': 449,\n",
       " 'curtains': 450,\n",
       " 'commercial': 451,\n",
       " 'strip': 452,\n",
       " 'laundry': 453,\n",
       " 'tile': 454,\n",
       " 'splash': 455,\n",
       " 'features': 456,\n",
       " 'ceiling': 457,\n",
       " 'fan': 458,\n",
       " 'marine': 459,\n",
       " 'cell': 460,\n",
       " 'empty': 461,\n",
       " 'storage': 462,\n",
       " 'wide': 463,\n",
       " 'buildings': 464,\n",
       " 'poster': 465,\n",
       " 'girls': 466,\n",
       " 'look': 467,\n",
       " 'pulling': 468,\n",
       " 'keyboard': 469,\n",
       " 'dish': 470,\n",
       " 'uniform': 471,\n",
       " 'smart': 472,\n",
       " 'jumbo': 473,\n",
       " 'jet': 474,\n",
       " 'touching': 475,\n",
       " 'slanted': 476,\n",
       " 'green': 477,\n",
       " 'urinals': 478,\n",
       " 'themed': 479,\n",
       " 'newly': 480,\n",
       " 'remodeled': 481,\n",
       " 'stalls': 482,\n",
       " 'installed': 483,\n",
       " 'along': 484,\n",
       " 'tiled': 485,\n",
       " 'stone': 486,\n",
       " 'tracks': 487,\n",
       " 'public': 488,\n",
       " 'over': 489,\n",
       " 'cozy': 490,\n",
       " 'could': 491,\n",
       " 'magazine': 492,\n",
       " 'service': 493,\n",
       " 'refrigerator': 494,\n",
       " 'high': 495,\n",
       " 'past': 496,\n",
       " 'furniture': 497,\n",
       " 'shop': 498,\n",
       " 'cow': 499,\n",
       " 'been': 500,\n",
       " 'basin': 501,\n",
       " 'only': 502,\n",
       " 'thing': 503,\n",
       " 'left': 504,\n",
       " 'spotted': 505,\n",
       " 'rail': 506,\n",
       " 'deep': 507,\n",
       " 'freezer': 508,\n",
       " 'displayed': 509,\n",
       " 'sinks': 510,\n",
       " 'towel': 511,\n",
       " 'soap': 512,\n",
       " 'dispensers': 513,\n",
       " 'stall': 514,\n",
       " 'building': 515,\n",
       " 'dishes': 516,\n",
       " 'under': 517,\n",
       " 'construction': 518,\n",
       " 'compact': 519,\n",
       " 'store': 520,\n",
       " 'passing': 521,\n",
       " 'bath': 522,\n",
       " 'tub': 523,\n",
       " 'tarmac': 524,\n",
       " 'baseball': 525,\n",
       " 'player': 526,\n",
       " 'bobble': 527,\n",
       " 'lights': 528,\n",
       " 'doll': 529,\n",
       " 'fancy': 530,\n",
       " 'ice': 531,\n",
       " 'chest': 532,\n",
       " 'focus': 533,\n",
       " 'unfinished': 534,\n",
       " 'pipes': 535,\n",
       " 'horse': 536,\n",
       " 'fridge': 537,\n",
       " 'magnets': 538,\n",
       " 'night': 539,\n",
       " 'lit': 540,\n",
       " 'dirt': 541,\n",
       " 'road': 542,\n",
       " 'forest': 543,\n",
       " 'base': 544,\n",
       " 'racers': 545,\n",
       " 'crowd': 546,\n",
       " 'rundown': 547,\n",
       " 'bathtub': 548,\n",
       " 'ocean': 549,\n",
       " 'walks': 550,\n",
       " 'big': 551,\n",
       " 'legs': 552,\n",
       " 'itself': 553,\n",
       " 'bend': 554,\n",
       " 'wooded': 555,\n",
       " 'complete': 556,\n",
       " 'cast': 557,\n",
       " 'iron': 558,\n",
       " 'motion': 559,\n",
       " 'blur': 560,\n",
       " 'gray': 561,\n",
       " 'tropical': 562,\n",
       " 'print': 563,\n",
       " 'shirt': 564,\n",
       " 'glasses': 565,\n",
       " 'hair': 566,\n",
       " 'hood': 567,\n",
       " 'spectators': 568,\n",
       " 'enjoying': 569,\n",
       " 'show': 570,\n",
       " 'eats': 571,\n",
       " 'instead': 572,\n",
       " 'pastries': 573,\n",
       " 'stream': 574,\n",
       " 'granite': 575,\n",
       " 'ride': 576,\n",
       " 'atv': 577,\n",
       " 'color': 578,\n",
       " 'fork': 579,\n",
       " 'rests': 580,\n",
       " 'carrying': 581,\n",
       " 'bright': 582,\n",
       " 'shining': 583,\n",
       " 'poles': 584,\n",
       " 'reflecting': 585,\n",
       " 'pavement': 586,\n",
       " 'chocolate': 587,\n",
       " 'bite': 588,\n",
       " 'half': 589,\n",
       " 'peeled': 590,\n",
       " 'upon': 591,\n",
       " 'doing': 592,\n",
       " 'stunt': 593,\n",
       " 'wheeler': 594,\n",
       " 'vehicles': 595,\n",
       " 'gold': 596,\n",
       " 'stainless': 597,\n",
       " 'can': 598,\n",
       " 'steel': 599,\n",
       " 'counters': 600,\n",
       " 'we': 601,\n",
       " 'clouds': 602,\n",
       " 'above': 603,\n",
       " 'coming': 604,\n",
       " 'objects': 605,\n",
       " 'herd': 606,\n",
       " 'traveling': 607,\n",
       " 'country': 608,\n",
       " 'surrounded': 609,\n",
       " 'lush': 610,\n",
       " 'landscape': 611,\n",
       " 'laying': 612,\n",
       " 'arms': 613,\n",
       " 'golden': 614,\n",
       " 'work': 615,\n",
       " 'painted': 616,\n",
       " 'oven': 617,\n",
       " 'not': 618,\n",
       " 'costume': 619,\n",
       " 'somebody': 620,\n",
       " 'adult': 621,\n",
       " 'vehicle': 622,\n",
       " 'clearly': 623,\n",
       " 'makes': 624,\n",
       " 'way': 625,\n",
       " 'hanging': 626,\n",
       " 'gate': 627,\n",
       " 'new': 628,\n",
       " 'goes': 629,\n",
       " 'great': 630,\n",
       " 'well': 631,\n",
       " 'decorated': 632,\n",
       " 'surroundings': 633,\n",
       " 'beach': 634,\n",
       " 'tent': 635,\n",
       " 'woods': 636,\n",
       " 'cop': 637,\n",
       " 'give': 638,\n",
       " 'ticket': 639,\n",
       " 'performing': 640,\n",
       " 'trick': 641,\n",
       " 'flock': 642,\n",
       " 'goats': 643,\n",
       " 'desert': 644,\n",
       " 'messing': 645,\n",
       " 'breakfast': 646,\n",
       " 'livestock': 647,\n",
       " 'habitat': 648,\n",
       " 'closeup': 649,\n",
       " 'herding': 650,\n",
       " 'apartment': 651,\n",
       " 'couple': 652,\n",
       " 'couches': 653,\n",
       " 'keep': 654,\n",
       " 'watch': 655,\n",
       " 'police': 656,\n",
       " 'officer': 657,\n",
       " 'speed': 658,\n",
       " 'writing': 659,\n",
       " 'windshield': 660,\n",
       " 'sleeps': 661,\n",
       " 'leaving': 662,\n",
       " 'trail': 663,\n",
       " 'living': 664,\n",
       " 'have': 665,\n",
       " 'tables': 666,\n",
       " 'chairs': 667,\n",
       " 'lone': 668,\n",
       " 'purple': 669,\n",
       " 'idle': 670,\n",
       " 'bush': 671,\n",
       " 'dozens': 672,\n",
       " 'metro': 673,\n",
       " 'running': 674,\n",
       " 'sheep': 675,\n",
       " 'rural': 676,\n",
       " 'walk': 677,\n",
       " 'busy': 678,\n",
       " 'tram': 679,\n",
       " 'make': 680,\n",
       " 'town': 681,\n",
       " 'straight': 682,\n",
       " 'leaves': 683,\n",
       " 'tail': 684,\n",
       " 'planes': 685,\n",
       " 'port': 686,\n",
       " 'run': 687,\n",
       " 'row': 688,\n",
       " 'silver': 689,\n",
       " 'railing': 690,\n",
       " 'keyboards': 691,\n",
       " 'parallel': 692,\n",
       " 'bunch': 693,\n",
       " 'crossing': 694,\n",
       " 'workers': 695,\n",
       " 'flight': 696,\n",
       " 'passes': 697,\n",
       " 'image': 698,\n",
       " 'hotel': 699,\n",
       " 'residential': 700,\n",
       " 'modern': 701,\n",
       " 'motel': 702,\n",
       " 'fixtures': 703,\n",
       " 'accessories': 704,\n",
       " 'runner': 705,\n",
       " 'come': 706,\n",
       " 'contains': 707,\n",
       " 'various': 708,\n",
       " 'devices': 709,\n",
       " 'doors': 710,\n",
       " 'museum': 711,\n",
       " 'war': 712,\n",
       " 'cleaned': 713,\n",
       " 'like': 714,\n",
       " 'stuffed': 715,\n",
       " 'eyes': 716,\n",
       " 'ten': 717,\n",
       " 'porcelain': 718,\n",
       " 'pieces': 719,\n",
       " 'floral': 720,\n",
       " 'patterns': 721,\n",
       " 'handles': 722,\n",
       " 'antique': 723,\n",
       " 'below': 724,\n",
       " 'illuminated': 725,\n",
       " 'collection': 726,\n",
       " 'indoor': 727,\n",
       " 'exhibit': 728,\n",
       " 'fine': 729,\n",
       " 'china': 730,\n",
       " 'touched': 731,\n",
       " 'sort': 732,\n",
       " 'facility': 733,\n",
       " 'possibly': 734,\n",
       " 'hanger': 735,\n",
       " 'staring': 736,\n",
       " 'tea': 737,\n",
       " 'bowls': 738,\n",
       " 'world': 739,\n",
       " 'airplanes': 740,\n",
       " 'lots': 741,\n",
       " 'paper': 742,\n",
       " 'holders': 743,\n",
       " 'rolls': 744,\n",
       " 'used': 745,\n",
       " 'roll': 746,\n",
       " 'dispenser': 747,\n",
       " 'commode': 748,\n",
       " 'tiny': 749,\n",
       " 'containers': 750,\n",
       " 'hang': 751,\n",
       " 'lay': 752,\n",
       " 'nice': 753,\n",
       " 'sets': 754,\n",
       " 'bar': 755,\n",
       " 'lid': 756,\n",
       " 'i': 757,\n",
       " 'built': 758,\n",
       " 'racing': 759,\n",
       " 'directly': 760,\n",
       " 'gathered': 761,\n",
       " 'dirty': 762,\n",
       " '3': 763,\n",
       " 'ladies': 764,\n",
       " 'lamp': 765,\n",
       " 'puddle': 766,\n",
       " 'van': 767,\n",
       " 'almost': 768,\n",
       " 'arched': 769,\n",
       " 'wood': 770,\n",
       " 'many': 771,\n",
       " 'perched': 772,\n",
       " 'concrete': 773,\n",
       " 'pretty': 774,\n",
       " 'church': 775,\n",
       " 'massive': 776,\n",
       " 'clock': 777,\n",
       " 'walkway': 778,\n",
       " 'onto': 779,\n",
       " 'narrow': 780,\n",
       " 'nearly': 781,\n",
       " 'resting': 782,\n",
       " 'spot': 783,\n",
       " 'something': 784,\n",
       " 'may': 785,\n",
       " 'number': 786,\n",
       " 'propeller': 787,\n",
       " 'kitten': 788,\n",
       " 'leftover': 789,\n",
       " 'identical': 790,\n",
       " 'photos': 791,\n",
       " 'coast': 792,\n",
       " 'fence': 793,\n",
       " 'preparing': 794,\n",
       " 'land': 795,\n",
       " 'highway': 796,\n",
       " 'slab': 797,\n",
       " 'playing': 798,\n",
       " 'height': 799,\n",
       " 'really': 800,\n",
       " 'screen': 801,\n",
       " 'embedded': 802,\n",
       " 'metallic': 803,\n",
       " 'camp': 804,\n",
       " 'sandy': 805,\n",
       " 'sad': 806,\n",
       " 'brush': 807,\n",
       " 'rim': 808,\n",
       " 'curb': 809,\n",
       " 'reflection': 810,\n",
       " 'rack': 811,\n",
       " 'seats': 812,\n",
       " 'directions': 813,\n",
       " 'mid': 814,\n",
       " 'roadside': 815,\n",
       " 'commuter': 816,\n",
       " 'cloth': 817,\n",
       " 'bananas': 818,\n",
       " 'himself': 819,\n",
       " 'shiny': 820,\n",
       " 'aerial': 821,\n",
       " 'someone': 822,\n",
       " 'football': 823,\n",
       " 'game': 824,\n",
       " 'slices': 825,\n",
       " 'bread': 826,\n",
       " 'camping': 827,\n",
       " 'tissue': 828,\n",
       " 'box': 829,\n",
       " 'pictures': 830,\n",
       " 'photographer': 831,\n",
       " 'shoes': 832,\n",
       " 'visible': 833,\n",
       " 'race': 834,\n",
       " 'calico': 835,\n",
       " 'drinking': 836,\n",
       " 'video': 837,\n",
       " 'contraption': 838,\n",
       " 'bundled': 839,\n",
       " 'winter': 840,\n",
       " 'clothing': 841,\n",
       " 'scarf': 842,\n",
       " 'cooking': 843,\n",
       " 'attempting': 844,\n",
       " 'get': 845,\n",
       " 'drink': 846,\n",
       " 'during': 847,\n",
       " 'competition': 848,\n",
       " 'away': 849,\n",
       " 'lady': 850,\n",
       " 'drinks': 851,\n",
       " 'island': 852,\n",
       " 'things': 853,\n",
       " 'trailer': 854,\n",
       " 'fighter': 855,\n",
       " 'arches': 856,\n",
       " 'helmet': 857,\n",
       " 'equipment': 858,\n",
       " 'wheels': 859,\n",
       " 'terminal': 860,\n",
       " 'marble': 861,\n",
       " 'shopping': 862,\n",
       " 'cart': 863,\n",
       " 'making': 864,\n",
       " 'turns': 865,\n",
       " 'painting': 866,\n",
       " 'homeless': 867,\n",
       " 'plant': 868,\n",
       " 'cattle': 869,\n",
       " 'steeple': 870,\n",
       " 'quiet': 871,\n",
       " 'hall': 872,\n",
       " 'he': 873,\n",
       " 'right': 874,\n",
       " 'floors': 875,\n",
       " 'cows': 876,\n",
       " 'thru': 877,\n",
       " 'sand': 878,\n",
       " 'featuring': 879,\n",
       " 'seating': 880,\n",
       " 'stools': 881,\n",
       " 'furnishings': 882,\n",
       " 'colors': 883,\n",
       " 'edge': 884,\n",
       " 'porch': 885,\n",
       " 'contemporary': 886,\n",
       " 'position': 887,\n",
       " 'cup': 888,\n",
       " 'round': 889,\n",
       " 'neat': 890,\n",
       " 'trim': 891,\n",
       " 'intently': 892,\n",
       " 'multi': 893,\n",
       " 'window': 894,\n",
       " 'plants': 895,\n",
       " 'distance': 896,\n",
       " 'bridge': 897,\n",
       " 'huge': 898,\n",
       " 'still': 899,\n",
       " 'sill': 900,\n",
       " 'frying': 901,\n",
       " 'indoors': 902,\n",
       " 'thick': 903,\n",
       " 'wheeled': 904,\n",
       " 'fly': 905,\n",
       " 'snowy': 906,\n",
       " 'curtain': 907,\n",
       " 'roof': 908,\n",
       " 'reads': 909,\n",
       " 'catching': 910,\n",
       " 'plumbing': 911,\n",
       " 'hardwood': 912,\n",
       " 'hats': 913,\n",
       " 'mounted': 914,\n",
       " 'circular': 915,\n",
       " 'tiles': 916,\n",
       " 'pattern': 917,\n",
       " 'jets': 918,\n",
       " 'against': 919,\n",
       " 'blowing': 920,\n",
       " 'candles': 921,\n",
       " 'party': 922,\n",
       " 'costumes': 923,\n",
       " 'showing': 924,\n",
       " 'clocks': 925,\n",
       " 'bell': 926,\n",
       " 'freeway': 927,\n",
       " 'overcast': 928,\n",
       " 'weather': 929,\n",
       " 'brushing': 930,\n",
       " 'teeth': 931,\n",
       " 'siting': 932,\n",
       " 'towed': 933,\n",
       " 'dreary': 934,\n",
       " 'containing': 935,\n",
       " 'bunches': 936,\n",
       " 'gas': 937,\n",
       " 'sign': 938,\n",
       " 'smiling': 939,\n",
       " 'scenic': 940,\n",
       " 'rock': 941,\n",
       " 'abandoned': 942,\n",
       " 'flooring': 943,\n",
       " 'prices': 944,\n",
       " 'matching': 945,\n",
       " 'rug': 946,\n",
       " 'skateboarding': 947,\n",
       " 'roadway': 948,\n",
       " 'downhill': 949,\n",
       " 'brushes': 950,\n",
       " 'bars': 951,\n",
       " 'male': 952,\n",
       " 'skateboarders': 953,\n",
       " 'rolling': 954,\n",
       " 'paved': 955,\n",
       " 'slope': 956,\n",
       " 'sold': 957,\n",
       " 'fire': 958,\n",
       " 'hydrant': 959,\n",
       " 'skate': 960,\n",
       " 'boards': 961,\n",
       " 'hilly': 962,\n",
       " 'skateboards': 963,\n",
       " 'luggage': 964,\n",
       " 'lies': 965,\n",
       " 'atm': 966,\n",
       " 'much': 967,\n",
       " 'bottle': 968,\n",
       " 'greenery': 969,\n",
       " 'tulips': 970,\n",
       " 'shows': 971,\n",
       " 'checkered': 972,\n",
       " 'fat': 973,\n",
       " 'deck': 974,\n",
       " 'dilapidated': 975,\n",
       " 'pool': 976,\n",
       " 'neon': 977,\n",
       " 'skateboard': 978,\n",
       " 'laughing': 979,\n",
       " 'attached': 980,\n",
       " 'smiles': 981,\n",
       " 'same': 982,\n",
       " 'elevated': 983,\n",
       " 'farmers': 984,\n",
       " 'market': 985,\n",
       " 'watermelon': 986,\n",
       " 'slice': 987,\n",
       " 'scooter': 988,\n",
       " 'pilot': 989,\n",
       " 'talking': 990,\n",
       " 'cluster': 991,\n",
       " 'towards': 992,\n",
       " 'ramp': 993,\n",
       " 't.v': 994,\n",
       " 'jumping': 995,\n",
       " 'reaching': 996,\n",
       " 'veggie': 997,\n",
       " 'blanket': 998,\n",
       " 'collar': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "caption = \"Hello how are you\"\n",
    "counter = defaultdict(int)\n",
    "tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "for token in tokens:\n",
    "    counter[token]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'hello': 1, 'how': 1, 'are': 1, 'you': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello 1\n",
      "how 1\n",
      "are 1\n",
      "you 1\n"
     ]
    }
   ],
   "source": [
    "for word, count in counter.items():\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Custom Dataset for COCO\n",
    "- Now that we have the vocabulary, we need to create a PyTorch custom Dataset class to load the images and captions. This class will:\n",
    "\n",
    "- Load the image from the COCO dataset.\n",
    "- Tokenize the caption using the vocabulary.\n",
    "- Apply transformations (like resizing and normalization) to the images.\n",
    "- Return the processed image and the tokenized caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import nltk\n",
    "from pycocotools.coco import COCO\n",
    "import random\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root, ann_file, vocab, transform=None, subset_fraction=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): Directory with all the images.\n",
    "            ann_file (string): Path to the annotation file.\n",
    "            vocab (Vocabulary): Vocabulary object for tokenizing captions.\n",
    "            transform (callable, optional): Transform to be applied to the images.\n",
    "            subset_fraction (float, optional): Fraction of dataset to use, e.g., 0.25 for 1/4th of the data.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.ids = list(self.coco.anns.keys())  # List of annotation IDs\n",
    "\n",
    "        # Use only a subset of the dataset if subset_fraction is less than 1.0\n",
    "        if subset_fraction < 1.0:\n",
    "            subset_size = int(len(self.ids) * subset_fraction)\n",
    "            self.ids = random.sample(self.ids, subset_size)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ann_id = self.ids[index]\n",
    "        caption = self.coco.anns[ann_id]['caption']\n",
    "        img_id = self.coco.anns[ann_id]['image_id']\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        path = img_info['file_name']\n",
    "        img_url = img_info['coco_url']  # Get the image URL from COCO annotations\n",
    "\n",
    "        # Fetch the image from the URL\n",
    "        response = requests.get(img_url)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "\n",
    "        # Load the image\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Tokenize the caption and convert to indices\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption_idx = [self.vocab('<start>')] + [self.vocab(token) for token in tokens] + [self.vocab('<end>')]\n",
    "        caption_tensor = torch.LongTensor(caption_idx)\n",
    "\n",
    "        return image, caption_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco = COCO('/home/mirsee/projects/image_captioning/notebooks/annotations/captions_val2017.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_id = list(coco.anns.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = coco.anns[ann_id]['caption']\n",
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = coco.anns[ann_id]['image_id']\n",
    "img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_info = coco.loadImgs(img_id)\n",
    "img_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_info = img_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = img_info['file_name']\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib.request\n",
    "img_url = img_info['coco_url']\n",
    "file_name = img_info['file_name']\n",
    "img_path = os.path.join('/home/mirsee/projects/image_captioning', file_name)\n",
    "urllib.request.urlretrieve(img_url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(caption.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_idx = [vocab('<start>')] +[vocab(token) in tokens]+[vocab('<end>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(caption_idx).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataLoader\n",
    "    Next, you need to create a DataLoader to load the dataset in mini-batches. \n",
    "    The DataLoader will shuffle the data and handle batching. However, captions are variable-length sequences, so you need a custom collate_fn to pad the captions in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "def collate_fn(data, pad_idx):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\"\"\"\n",
    "    # Sort data by caption length (descending)\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Stack images into a single tensor\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Pad the captions to the maximum length in the batch\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    padded_captions = rnn.pad_sequence(captions, batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "    return images, padded_captions, lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Let’s break down the `collate_fn` function step by step, using an example to make it more intuitive.\n",
    "\n",
    "### Purpose of `collate_fn`\n",
    "\n",
    "When you load data for a neural network in batches (e.g., 32 images at a time), the captions associated with each image can have different lengths. Since neural networks expect inputs to have the same size (tensors of fixed dimensions), we need to:\n",
    "1. **Sort captions**: Arrange the captions by length (optional but helpful for certain models).\n",
    "2. **Pad captions**: Ensure that all captions in the batch have the same length by padding shorter captions with a special padding token (`<pad>`).\n",
    "3. **Return batch tensors**: Return the images, padded captions, and the lengths of the original (unpadded) captions so the model knows how long each caption is.\n",
    "\n",
    "### Breakdown of `collate_fn`\n",
    "\n",
    "1. **Input**: The function takes two arguments:\n",
    "   - `data`: A list of tuples where each tuple contains an image tensor and a caption tensor. The structure is `[(image1, caption1), (image2, caption2), ...]`.\n",
    "   - `pad_idx`: The index of the padding token (`<pad>`) in the vocabulary. This will be used to pad the shorter captions.\n",
    "\n",
    "2. **Sorting Data by Caption Length**:\n",
    "   ```python\n",
    "   data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "   ```\n",
    "   This sorts the list of (image, caption) tuples by the length of the captions, in descending order. Sorting by length is often done because some models (e.g., RNNs) can benefit from processing longer sequences first.\n",
    "\n",
    "3. **Separate Images and Captions**:\n",
    "   ```python\n",
    "   images, captions = zip(*data)\n",
    "   ```\n",
    "   This line splits the `data` list into two separate lists:\n",
    "   - `images`: Contains all the images in the batch.\n",
    "   - `captions`: Contains all the corresponding captions.\n",
    "\n",
    "4. **Stack Images**:\n",
    "   ```python\n",
    "   images = torch.stack(images, 0)\n",
    "   ```\n",
    "   This stacks all the image tensors into a single batch tensor. If each image has a shape of `(3, 224, 224)` (3 color channels, 224x224 pixels), and the batch size is 4, the resulting tensor will have a shape of `(4, 3, 224, 224)`.\n",
    "\n",
    "5. **Pad Captions**:\n",
    "   ```python\n",
    "   padded_captions = rnn.pad_sequence(captions, batch_first=True, padding_value=pad_idx)\n",
    "   ```\n",
    "   The `pad_sequence` function pads all the captions in the batch to the length of the longest caption, using the provided `pad_idx`. The shorter captions are padded with the `<pad>` token until they reach the length of the longest caption in the batch.\n",
    "\n",
    "6. **Get Original Caption Lengths**:\n",
    "   ```python\n",
    "   lengths = [len(cap) for cap in captions]\n",
    "   ```\n",
    "   This stores the original (unpadded) length of each caption in a list. These lengths are useful when processing the captions in RNNs or LSTMs, where you want to ignore the padding during training.\n",
    "\n",
    "7. **Return Values**:\n",
    "   The function returns three items:\n",
    "   - `images`: A tensor of stacked images.\n",
    "   - `padded_captions`: A tensor of captions padded to the same length.\n",
    "   - `lengths`: The original lengths of each caption before padding.\n",
    "\n",
    "### Example Walkthrough\n",
    "\n",
    "Let’s walk through an example with 3 images and their associated captions.\n",
    "\n",
    "#### Data (Input to `collate_fn`):\n",
    "\n",
    "Let’s say we have three image-caption pairs:\n",
    "- **Image 1**: Tensor of shape `(3, 224, 224)`, caption: `['a', 'dog', 'is', 'running']`\n",
    "- **Image 2**: Tensor of shape `(3, 224, 224)`, caption: `['a', 'cat']`\n",
    "- **Image 3**: Tensor of shape `(3, 224, 224)`, caption: `['a', 'bird', 'is', 'flying', 'away']`\n",
    "\n",
    "The captions can be converted to indices using a vocabulary. Suppose:\n",
    "- `'a'` = 2\n",
    "- `'dog'` = 3\n",
    "- `'is'` = 4\n",
    "- `'running'` = 5\n",
    "- `'cat'` = 6\n",
    "- `'bird'` = 7\n",
    "- `'flying'` = 8\n",
    "- `'away'` = 9\n",
    "- `<pad>` = 0\n",
    "\n",
    "The input `data` to the `collate_fn` function will look like:\n",
    "```python\n",
    "[\n",
    "    (image1, [2, 3, 4, 5]),   # \"a dog is running\"\n",
    "    (image2, [2, 6]),         # \"a cat\"\n",
    "    (image3, [2, 7, 4, 8, 9]) # \"a bird is flying away\"\n",
    "]\n",
    "```\n",
    "\n",
    "#### Step-by-Step Execution:\n",
    "\n",
    "1. **Sort by Caption Length**:\n",
    "   After sorting, the data will be ordered as:\n",
    "   ```python\n",
    "   [\n",
    "       (image3, [2, 7, 4, 8, 9]), # Longest: \"a bird is flying away\"\n",
    "       (image1, [2, 3, 4, 5]),    # Medium: \"a dog is running\"\n",
    "       (image2, [2, 6])           # Shortest: \"a cat\"\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "2. **Separate Images and Captions**:\n",
    "   ```python\n",
    "   images = [image3, image1, image2]\n",
    "   captions = [[2, 7, 4, 8, 9], [2, 3, 4, 5], [2, 6]]\n",
    "   ```\n",
    "\n",
    "3. **Stack Images**:\n",
    "   The images are stacked into a batch:\n",
    "   ```python\n",
    "   images.shape = (3, 3, 224, 224)  # 3 images, each with 3 channels (RGB), and 224x224 pixels\n",
    "   ```\n",
    "\n",
    "4. **Pad Captions**:\n",
    "   We pad all the captions to the length of the longest caption (which is 5 words: `\"a bird is flying away\"`). After padding:\n",
    "   ```python\n",
    "   padded_captions = [\n",
    "       [2, 7, 4, 8, 9],  # \"a bird is flying away\" (no padding needed)\n",
    "       [2, 3, 4, 5, 0],  # \"a dog is running\" -> padded with <pad> token (0)\n",
    "       [2, 6, 0, 0, 0]   # \"a cat\" -> padded with <pad> token (0)\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "   The resulting tensor will have the shape `(3, 5)`.\n",
    "\n",
    "5. **Get Original Caption Lengths**:\n",
    "   ```python\n",
    "   lengths = [5, 4, 2]\n",
    "   ```\n",
    "\n",
    "#### Final Output (What `collate_fn` Returns):\n",
    "\n",
    "- **`images`**: Tensor of shape `(3, 3, 224, 224)` (the 3 images stacked together).\n",
    "- **`padded_captions`**: Tensor of shape `(3, 5)`:\n",
    "  ```python\n",
    "  [\n",
    "      [2, 7, 4, 8, 9],  # \"a bird is flying away\"\n",
    "      [2, 3, 4, 5, 0],  # \"a dog is running\"\n",
    "      [2, 6, 0, 0, 0]   # \"a cat\"\n",
    "  ]\n",
    "  ```\n",
    "- **`lengths`**: The original lengths of the captions, i.e., `[5, 4, 2]`.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "The `collate_fn` function:\n",
    "- Sorts the data by caption length.\n",
    "- Pads shorter captions so all captions in the batch have the same length.\n",
    "- Returns the batch of images, padded captions, and the original lengths of the captions.\n",
    "\n",
    "This is essential for efficient batch processing of captions of different lengths during training. The model can now handle batches of variable-length captions by ignoring the padded values during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to COCO image folder and annotations\n",
    "image_root = '/home/mirsee/projects/image_captioning/data'\n",
    "ann_file = '/home/mirsee/projects/image_captioning/notebooks/annotations/captions_train2017.json'\n",
    "\n",
    "# Build vocabulary (done earlier)\n",
    "vocab = build_vocab(ann_file, threshold=5)\n",
    "\n",
    "# Create the dataset with only 1/4 of the COCO dataset\n",
    "subset_fraction = 0.25  # Use only 25% of the dataset\n",
    "dataset = CocoDataset(root=image_root, ann_file=ann_file, vocab=vocab, transform=transform, subset_fraction=subset_fraction)\n",
    "\n",
    "pad_idx = vocab('<pad>')\n",
    "\n",
    "# Create the DataLoader, passing the pad_idx to the collate_fn\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, \n",
    "                         num_workers=4, collate_fn=lambda x: collate_fn(x, pad_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the data loader\n",
    "for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "    print(\"Batch\", i)\n",
    "    print(\"Images shape:\", images.shape)  # (batch_size, 3, 224, 224)\n",
    "    print(\"Captions shape:\", captions.shape)  # (batch_size, max_caption_length)\n",
    "    print(\"Lengths:\", lengths)\n",
    "\n",
    "    if i == 0:  # Only show the first batch\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It looks like your DataLoader is now working correctly. Here’s a quick summary of what you have:\n",
    "\n",
    "Output Explanation:\n",
    "Batch 0: This is the first mini-batch of data.\n",
    "\n",
    "Images shape: torch.Size([32, 3, 224, 224])\n",
    "\n",
    "You have a batch size of 32 images.\n",
    "Each image is in 3 channels (RGB).\n",
    "Each image has been resized to 224x224 pixels (due to your transformation).\n",
    "Captions shape: torch.Size([32, 23])\n",
    "\n",
    "There are 32 captions in this batch (matching the batch size).\n",
    "The maximum caption length in this batch is 23 tokens.\n",
    "All captions have been padded to the maximum length of 23 tokens in the batch.\n",
    "Lengths: [23, 19, 19, 17, 17, ...]\n",
    "\n",
    "This is the list of actual lengths of the captions before padding.\n",
    "The longest caption in the batch has 23 tokens, while the shortest has 10 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transform\n",
    "import torch\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import nltk\n",
    "from pycocotools.coco import COCO\n",
    "import random\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, threshold):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # Special Tokesn\n",
    "        self.add_word('<pad>')\n",
    "        self.add_word('<start>')\n",
    "        self.add_word('<end>')\n",
    "        self.add_word('<unk>')\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx+=1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the vocabulary.\"\"\"\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(ann_file, threshold=5):\n",
    "    from pycocotools.coco import COCO\n",
    "    coco = COCO(ann_file)\n",
    "    counter = defaultdict(int)\n",
    "    for ann_id in coco.anns.keys():\n",
    "        caption = coco.anns[ann_id]['caption']\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        for token in tokens:\n",
    "            counter[token]+=1\n",
    "\n",
    "    # create vocab\n",
    "    vocab = Vocabulary(threshold)\n",
    "    for word, count in counter.items():\n",
    "        if count>threshold:\n",
    "            vocab.add_word(word)\n",
    "\n",
    "    return vocab\n",
    "    \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    " \n",
    "\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root, ann_file, vocab, transform=None, subset_fraction=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): Directory with all the images.\n",
    "            ann_file (string): Path to the annotation file.\n",
    "            vocab (Vocabulary): Vocabulary object for tokenizing captions.\n",
    "            transform (callable, optional): Transform to be applied to the images.\n",
    "            subset_fraction (float, optional): Fraction of dataset to use, e.g., 0.25 for 1/4th of the data.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.ids = list(self.coco.anns.keys())  # List of annotation IDs\n",
    "\n",
    "        # Use only a subset of the dataset if subset_fraction is less than 1.0\n",
    "        if subset_fraction < 1.0:\n",
    "            subset_size = int(len(self.ids) * subset_fraction)\n",
    "            self.ids = random.sample(self.ids, subset_size)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ann_id = self.ids[index]\n",
    "        caption = self.coco.anns[ann_id]['caption']\n",
    "        img_id = self.coco.anns[ann_id]['image_id']\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        path = img_info['file_name']\n",
    "        img_url = img_info['coco_url']  # Get the image URL from COCO annotations\n",
    "\n",
    "        # Fetch the image from the URL\n",
    "        response = requests.get(img_url)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "\n",
    "        # Load the image\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Tokenize the caption and convert to indices\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption_idx = [self.vocab('<start>')] + [self.vocab(token) for token in tokens] + [self.vocab('<end>')]\n",
    "        caption_tensor = torch.LongTensor(caption_idx)\n",
    "\n",
    "        return image, caption_tensor\n",
    "   \n",
    "# Path to COCO image folder and annotations\n",
    "image_root = '/home/mirsee/projects/image_captioning/data'\n",
    "ann_file = '/home/mirsee/projects/image_captioning/notebooks/annotations/captions_train2017.json'\n",
    "\n",
    "# Build vocabulary (done earlier)\n",
    "vocab = build_vocab(ann_file, threshold=5)\n",
    "\n",
    "# Create the dataset with only 1/4 of the COCO dataset\n",
    "subset_fraction = 0.25  # Use only 25% of the dataset\n",
    "dataset = CocoDataset(root=image_root, ann_file=ann_file, vocab=vocab, transform=transform, subset_fraction=subset_fraction)\n",
    "\n",
    "pad_idx = vocab('<pad>')\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "def collate_fn(data, pad_idx):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\"\"\"\n",
    "    # Sort data by caption length (descending)\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Stack images into a single tensor\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Pad the captions to the maximum length in the batch\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    padded_captions = rnn.pad_sequence(captions, batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "    return images, padded_captions, lengths\n",
    "\n",
    "\n",
    "# Create the DataLoader, passing the pad_idx to the collate_fn\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, \n",
    "                         num_workers=4, collate_fn=lambda x: collate_fn(x, pad_idx))\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    takes in the size of the embeded_vector to fed to rnn.\n",
    "    this is not used for training but just get the feature vector of size embed size\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True) #Load the model with all the pretrained weights\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "            #By setting requires_grad=False, you are telling PyTorch not to compute gradients for this tensor during backpropagation.\n",
    "    \n",
    "    # get all the layers except last as we are not intereseted in classification\n",
    "        modules = list(resnet.children())[:-1] # last layer Linear(in_features=2048, out_features=1000, bias=True)\n",
    "        self.resnet = nn.Sequential(*modules)#unpackign the layers\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1) # flatten the layer \n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "    \n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # Embedding layer: converts word indices into dense vectors of size embed_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # LSTM: input to hidden, hidden_size must match the size of features from CNN\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to map LSTM output to vocab_size\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Initialize the hidden state (if needed)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Optional dropout to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "        Arguments:\n",
    "        - features: Tensor of shape (batch_size, feature_size=512)\n",
    "        - captions: Tensor of shape (batch_size, max_caption_length), word indices\n",
    "        \n",
    "        Returns:\n",
    "        - outputs: Tensor of shape (batch_size, max_caption_length, vocab_size), word predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        # Embedding the captions, excluding the <end> token\"\n",
    "        embeddings = self.embedding(captions[:, :-1])\n",
    "        \n",
    "        # Concatenate the features with the embedded captions\n",
    "        # Features are passed as input to the first time step\n",
    "        features = features.unsqueeze(1)  # shape (batch_size, 1, feature_size)\n",
    "        lstm_input = torch.cat((features, embeddings), 1)  # shape (batch_size, 1 + caption_length, embed_size)\n",
    "        \n",
    "        # Pass the concatenated inputs through the LSTM\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        \n",
    "        # Pass the LSTM output through the fully connected layer to get word predictions\n",
    "        outputs = self.fc(lstm_out)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab)  # Vocabulary size (make sure vocab.__len__() is implemented)\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "log_interval = 10  # Log every 10 batches\n",
    "batch_size = 32  # Batch size for training\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Initialize models\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers=1).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
    "\n",
    "# Load data\n",
    "# Assume you have already created a DataLoader called data_loader\n",
    "# data_loader = DataLoader(...)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()  # Set encoder to training mode\n",
    "    decoder.train()  # Set decoder to training mode\n",
    "    \n",
    "    total_loss = 0  # Keep track of loss for the entire epoch\n",
    "    \n",
    "    for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "        \n",
    "        # Move images and captions to the device (GPU or CPU)\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Forward pass: Extract image features using the encoder\n",
    "        features = encoder(images)  # (batch_size, embed_size)\n",
    "\n",
    "        # Forward pass: Generate captions using the decoder\n",
    "        outputs = decoder(features, captions)  # (batch_size, max_caption_length, vocab_size)\n",
    "        \n",
    "        # Adjust the outputs to exclude the last time step, to match the target captions[:, 1:]\n",
    "        outputs = outputs[:, :-1, :]  # Exclude the last predicted word\n",
    "\n",
    "# Flatten the outputs and the target captions\n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions[:, 1:].contiguous().view(-1))\n",
    "\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model's parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Logging the loss every log_interval\n",
    "        if i % log_interval == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(data_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Average loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / len(data_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
